{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1ab8f9-2947-44fc-9de6-bd741c27eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "from parser.utils import load_json, dfs_cardinality, estimate_scan_in_mb\n",
    "from models.feature.single_xgboost_feature import find_top_k_operators, featurize_one_plan, get_top_k_table_by_size\n",
    "from utils.load_brad_trace import load_trace, create_concurrency_dataset, load_trace_all_version\n",
    "from models.concurrency.utils import pre_info_train_test_seperation\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from models.single.stage import SingleStage\n",
    "from models.concurrency.complex_models import ConcurrentRNN\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adcb1507-a2fc-4323-a350-9566aee3b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from utils.load_brad_trace import (\n",
    "    load_trace,\n",
    "    create_concurrency_dataset,\n",
    "    load_trace_all_version,\n",
    ")\n",
    "from models.single.stage import SingleStage\n",
    "from models.concurrency.complex_models import ConcurrentRNN\n",
    "from scheduler.base_scheduler import BaseScheduler\n",
    "\n",
    "\n",
    "class QueryBank:\n",
    "    def __init__(\n",
    "        self, sql_query_file: str, query_runtime_path: str, seed: int = 0\n",
    "    ) -> None:\n",
    "        with open(sql_query_file, \"r\") as f:\n",
    "            sql_queries = f.readlines()\n",
    "        query_runtime = np.load(query_runtime_path)\n",
    "        assert len(sql_queries) == len(query_runtime)\n",
    "        idx = np.argsort(query_runtime)\n",
    "        self.query_runtime = query_runtime[idx]\n",
    "        self.sql_queries = [sql_queries[i] for i in idx]\n",
    "        self.query_len = len(self.query_runtime)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def random_sample(self) -> (str, float):\n",
    "        # make a random sample of the query\n",
    "        idx = np.random.randint(self.query_len)\n",
    "        return self.sql_queries[idx], self.query_runtime[idx]\n",
    "\n",
    "    def sample_by_runtime(self, runtime: float) -> (str, float):\n",
    "        # sample a query that best matches the runtime\n",
    "        idx = np.searchsorted(self.query_runtime, runtime)\n",
    "        idx = max(idx, self.query_len - 1)\n",
    "        return self.sql_queries[idx], self.query_runtime[idx]\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(\n",
    "        self, scheduler: BaseScheduler, query_bank: Optional[QueryBank] = None, pause_wait_s: float = 1.0\n",
    "    ):\n",
    "        self.scheduler = scheduler\n",
    "        self.query_bank = query_bank\n",
    "        self.pause_wait_s = pause_wait_s\n",
    "\n",
    "    def replay_one_query(self, start_time: float, next_query_start_time: Optional[float] = None,\n",
    "                         query_str: Optional[int] = None, query_idx: Optional[int] = None):\n",
    "        # Todo: this logical should go to the scheduler\n",
    "        should_immediate_re_ingest, should_pause_and_re_ingest = self.scheduler.ingest_query_simulation(\n",
    "            start_time, query_str=query_str, query_idx=query_idx\n",
    "        )\n",
    "        print(should_immediate_re_ingest, should_pause_and_re_ingest)\n",
    "        if should_immediate_re_ingest:\n",
    "            # the scheduler schedules one query at a time even if there are multiple queries in the queue, so need to call again\n",
    "            self.replay_one_query(start_time + 0.001)\n",
    "        if should_pause_and_re_ingest:\n",
    "            if next_query_start_time is not None and next_query_start_time <= start_time + self.pause_wait_s:\n",
    "                return\n",
    "            self.replay_one_query(start_time + self.pause_wait_s)\n",
    "\n",
    "    def replay_workload(self, directory: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        all_raw_trace, all_trace = load_trace(directory, 8, concat=True)\n",
    "        concurrency_df = create_concurrency_dataset(all_trace, engine=None, pre_exec_interval=200)\n",
    "        concurrency_df = concurrency_df.sort_values(by=['start_time'], ascending=True)\n",
    "        original_predictions = self.scheduler.make_original_prediction(concurrency_df)\n",
    "        assert len(concurrency_df) == len(original_predictions)\n",
    "        original_runtime = []\n",
    "        all_start_time = concurrency_df[\"start_time\"].values\n",
    "        all_query_idx = concurrency_df[\"query_idx\"].values\n",
    "        for i in range(len(concurrency_df)):\n",
    "            original_runtime.append(original_predictions[i])\n",
    "            # replaying the query one-by-one\n",
    "            if i < len(concurrency_df):\n",
    "                next_query_start_time = all_start_time[i + 1]\n",
    "            else:\n",
    "                next_query_start_time = None\n",
    "            self.replay_one_query(all_start_time[i], next_query_start_time, i, all_query_idx[i])\n",
    "        # finish all queries\n",
    "        self.scheduler.finish_query(np.infty)\n",
    "        new_runtime = []\n",
    "        for i in range(len(concurrency_df)):\n",
    "            new_runtime.append(self.scheduler.all_query_runtime[i])\n",
    "        original_runtime = np.asarray(original_runtime)\n",
    "        new_runtime = np.asarray(new_runtime)\n",
    "        return original_runtime, new_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90ce284a-0421-456f-b645-8acd9cf01d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional, Tuple, List, Union, MutableMapping\n",
    "from models.single.stage import SingleStage\n",
    "from models.concurrency.complex_models import ConcurrentRNN\n",
    "\n",
    "\n",
    "def reverse_index_list(lst: List, pop_index: List[int]) -> List:\n",
    "    return [\n",
    "        lst[i] for i in range(len(lst)) if i not in pop_index\n",
    "    ]\n",
    "\n",
    "\n",
    "class BaseScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        stage_model: SingleStage,\n",
    "        predictor: ConcurrentRNN,\n",
    "        max_concurrency_level: int = 5,\n",
    "    ):\n",
    "        self.stage_model = stage_model\n",
    "        self.predictor = predictor\n",
    "        self.max_concurrency_level = max_concurrency_level\n",
    "\n",
    "        self.existing_query_features: List[np.ndarray] = []\n",
    "        self.existing_query_concur_features: List[Optional[torch.Tensor]] = []\n",
    "        self.existing_pre_info_length: List[int] = []\n",
    "        self.existing_start_time: List[float] = []\n",
    "        self.existing_finish_time: List[float] = []\n",
    "        self.existing_runtime_prediction_dict: MutableMapping[Union[str, int], float] = dict()\n",
    "        self.existing_runtime_prediction: List[float] = []\n",
    "        self.queued_query_features: List[np.ndarray] = []\n",
    "        self.current_time = 0\n",
    "        self.running_queries: Union[List[str], List[int]] = []\n",
    "        self.queued_queries: Union[List[str], List[int]] = []\n",
    "        self.existing_enter_time: List[float] = []\n",
    "        self.queued_queries_enter_time: List[float] = []\n",
    "        self.all_query_runtime: MutableMapping[Union[str, int], float] = dict()\n",
    "\n",
    "    def make_original_prediction(self, trace: pd.DataFrame) -> np.ndarray:\n",
    "        all_pred, _ = self.predictor.predict(trace, return_per_query=False)\n",
    "        return all_pred\n",
    "\n",
    "    def ingest_query(self, start_t: float, query_idx: int):\n",
    "        return None\n",
    "\n",
    "    def print_state(self):\n",
    "        print(\"current time: \", self.current_time)\n",
    "        print(\"running_queries: \", list(zip(self.running_queries, self.existing_runtime_prediction)))\n",
    "        print(\"queued_queries: \", self.queued_queries)\n",
    "\n",
    "    def submit_query(\n",
    "        self,\n",
    "        pos_in_queue: int,\n",
    "        query_rep: Union[str, int],\n",
    "        pred_runtime: float,\n",
    "        query_feature: np.ndarray,\n",
    "        submit_time: float,\n",
    "        enter_time: float,\n",
    "        finish_t: float,\n",
    "        query_concur_features: Optional[torch.Tensor],\n",
    "        pre_info_length: int,\n",
    "        new_existing_finish_time: Optional[List[float]] = None,\n",
    "        new_existing_runtime_prediction: Optional[List[float]] = None,\n",
    "        new_existing_query_concur_features: Optional[List[Optional[torch.Tensor]]] = None\n",
    "    ):\n",
    "        # first upload the prediction on existing runtime when a new query is submitted\n",
    "        if new_existing_finish_time is not None:\n",
    "            self.existing_finish_time = new_existing_finish_time\n",
    "        if new_existing_runtime_prediction is not None:\n",
    "            self.existing_runtime_prediction = new_existing_runtime_prediction\n",
    "        if new_existing_query_concur_features is not None:\n",
    "            self.existing_query_concur_features = new_existing_query_concur_features\n",
    "        self.running_queries.append(query_rep)\n",
    "        self.existing_query_features.append(query_feature)\n",
    "        self.existing_start_time.append(submit_time)\n",
    "        self.existing_finish_time.append(finish_t)\n",
    "        self.existing_query_concur_features.append(query_concur_features)\n",
    "        self.existing_pre_info_length.append(pre_info_length)\n",
    "        self.existing_enter_time.append(enter_time)\n",
    "        self.existing_runtime_prediction.append(pred_runtime)\n",
    "        self.queued_queries.pop(pos_in_queue)\n",
    "        self.queued_query_features.pop(pos_in_queue)\n",
    "        self.queued_queries_enter_time.pop(pos_in_queue)\n",
    "\n",
    "\n",
    "    def finish_query(self, current_time: float = None) -> None:\n",
    "        if current_time is not None:\n",
    "            self.current_time = current_time\n",
    "        pop_index = []\n",
    "        for i, finish_t in enumerate(self.existing_finish_time):\n",
    "            if finish_t <= self.current_time:\n",
    "                pop_index.append(i)\n",
    "                query_str = self.running_queries[i]\n",
    "                self.all_query_runtime[query_str] = (\n",
    "                    finish_t - self.existing_enter_time[i]\n",
    "                )\n",
    "        if len(pop_index) == 0:\n",
    "            return\n",
    "        length = len(self.existing_finish_time)\n",
    "        self.running_queries = reverse_index_list(self.running_queries, pop_index)\n",
    "        self.existing_enter_time = reverse_index_list(self.existing_enter_time, pop_index)\n",
    "        self.existing_query_features = reverse_index_list(self.existing_query_features, pop_index)\n",
    "        self.existing_runtime_prediction = reverse_index_list(self.existing_runtime_prediction, pop_index)\n",
    "        self.existing_start_time = reverse_index_list(self.existing_start_time, pop_index)\n",
    "        self.existing_finish_time = reverse_index_list(self.existing_finish_time, pop_index)\n",
    "        # Todo: the last two needs change when we remove a query from its pre info,\n",
    "        #  or we train with sufficient squence length\n",
    "        self.existing_query_concur_features = [\n",
    "            self.existing_query_concur_features[i]\n",
    "            for i in range(length)\n",
    "            if i not in pop_index\n",
    "        ]\n",
    "        self.existing_pre_info_length = [\n",
    "            self.existing_pre_info_length[i]\n",
    "            for i in range(length)\n",
    "            if i not in pop_index\n",
    "        ]\n",
    "\n",
    "    def ingest_query_simulation(\n",
    "        self,\n",
    "        start_t: float,\n",
    "        query_str: Optional[Union[str, int]] = None,\n",
    "        query_idx: Optional[int] = None,\n",
    "    ) -> Tuple[bool, bool]:\n",
    "        \"\"\"We work on planning the currently queued queries if quert_str is None (i.e., no query submitted)\"\"\"\n",
    "        self.current_time = start_t\n",
    "        self.finish_query()\n",
    "        should_immediate_re_ingest = False\n",
    "        should_pause_and_re_ingest = False\n",
    "        if query_str is not None:\n",
    "            self.queued_queries.append(query_str)\n",
    "            self.queued_queries_enter_time.append(start_t)\n",
    "            query_feature = self.stage_model.featurize_online(query_idx)\n",
    "            self.queued_query_features.append(query_feature)\n",
    "\n",
    "        if len(self.queued_query_features) == 0:\n",
    "            # nothing to do when there is no query in the queue\n",
    "            return should_immediate_re_ingest, should_pause_and_re_ingest\n",
    "\n",
    "        predictions, global_x, global_pre_info_length = self.predictor.online_inference(\n",
    "            self.existing_query_features,\n",
    "            self.existing_query_concur_features,\n",
    "            self.existing_pre_info_length,\n",
    "            self.queued_query_features,\n",
    "            self.existing_start_time,\n",
    "            start_t,\n",
    "        )\n",
    "\n",
    "        predictions = predictions.reshape(-1).detach().numpy()\n",
    "        # Todo: add algorithms to decide whether to put in queue or directly for execution\n",
    "        if len(self.running_queries) == 0:\n",
    "            # submit up to self.max_concurrency_level number of queries in queue when there is no query running\n",
    "            # Todo: this is not optimal\n",
    "            assert len(predictions) == len(self.queued_queries)\n",
    "            sort_idx = np.argsort(predictions)\n",
    "            if len(sort_idx) >= self.max_concurrency_level:\n",
    "                sort_idx = sort_idx[: self.max_concurrency_level]\n",
    "            submit_query_str = []\n",
    "            submit_query_feature = []\n",
    "            submit_enter_time = []\n",
    "            submit_pred_runtime = []\n",
    "            for i in sort_idx:\n",
    "                submit_query_str.append(self.queued_queries[i])\n",
    "                submit_query_feature.append(self.queued_query_features[i])\n",
    "                submit_enter_time.append(self.queued_queries_enter_time[i])\n",
    "                submit_pred_runtime.append(float(predictions[i]))\n",
    "            for i, idx in enumerate(sort_idx):\n",
    "                finish_t = float(predictions[idx]) + start_t\n",
    "                query_str = submit_query_str[i]\n",
    "                query_feature = submit_query_feature[i]\n",
    "                enter_t = submit_enter_time[i]\n",
    "                pred_runtime = submit_pred_runtime[i]\n",
    "                self.submit_query(\n",
    "                    idx,\n",
    "                    query_str,\n",
    "                    pred_runtime,\n",
    "                    query_feature,\n",
    "                    start_t,\n",
    "                    enter_t,\n",
    "                    finish_t,\n",
    "                    None,\n",
    "                    int(global_pre_info_length[idx]),\n",
    "                )\n",
    "            return should_immediate_re_ingest, should_pause_and_re_ingest\n",
    "        elif len(self.running_queries) >= self.max_concurrency_level:\n",
    "            # when the server is running at its full capacity, should pause and retry\n",
    "            should_pause_and_re_ingest = True\n",
    "            return should_immediate_re_ingest, should_pause_and_re_ingest\n",
    "        else:\n",
    "            # Todo: implement some better algos\n",
    "            # Todo: add another logic: if the currently queued queries are all \"bad\" for the system load, pause and retry\n",
    "            all_new_existing_pred = []\n",
    "            all_curr_pred = []\n",
    "            all_delta_sum = []\n",
    "            all_query_concur_feature = []\n",
    "            all_global_pre_info_length = []\n",
    "            all_existing_query_concur_feature = []\n",
    "            for i in range(len(self.queued_queries)):\n",
    "                pred_idx = i * (1 + len(self.existing_query_concur_features))\n",
    "                all_global_pre_info_length.append(global_pre_info_length[pred_idx])\n",
    "                curr_pred = predictions[pred_idx]\n",
    "                curr_concur_feature = global_x[pred_idx]\n",
    "                all_curr_pred.append(curr_pred)\n",
    "                old_existing_pred = np.asarray(self.existing_runtime_prediction)\n",
    "                new_existing_pred = predictions[(pred_idx + 1): (pred_idx + len(self.existing_query_concur_features) + 1)]\n",
    "                curr_existing_query_concur_feature = []\n",
    "                for j in range(pred_idx + 1, pred_idx + len(self.existing_query_concur_features) + 1):\n",
    "                    curr_existing_query_concur_feature.append(global_x[j])\n",
    "                all_new_existing_pred.append(new_existing_pred)\n",
    "                all_query_concur_feature.append(curr_concur_feature)\n",
    "                all_existing_query_concur_feature.append(curr_existing_query_concur_feature)\n",
    "                # realistically, should be a positive number, the smaller, the better\n",
    "                delta = new_existing_pred - old_existing_pred\n",
    "                all_delta_sum.append(np.sum(delta))\n",
    "            # Heuristic to submit the query that incur minimal delta on the existing queries, then resubmit the next\n",
    "            selected_idx = np.argmin(all_delta_sum)\n",
    "            finish_t = all_curr_pred[selected_idx] + start_t\n",
    "            new_existing_finish_time = []\n",
    "            for i in range(len(self.existing_start_time)):\n",
    "                new_existing_finish_time.append(all_new_existing_pred[selected_idx][i] + self.existing_start_time[i])\n",
    "            self.submit_query(\n",
    "                selected_idx,\n",
    "                self.queued_queries[selected_idx],\n",
    "                all_curr_pred[selected_idx],\n",
    "                self.queued_query_features[selected_idx],\n",
    "                start_t,\n",
    "                self.queued_queries_enter_time[selected_idx],\n",
    "                finish_t,\n",
    "                all_query_concur_feature[selected_idx],\n",
    "                int(global_pre_info_length[selected_idx]),\n",
    "                new_existing_finish_time,\n",
    "                list(all_new_existing_pred[selected_idx]),\n",
    "                all_existing_query_concur_feature[selected_idx]\n",
    "            )\n",
    "            # immediately resubmit the next\n",
    "            should_immediate_re_ingest = True\n",
    "            return should_immediate_re_ingest, should_pause_and_re_ingest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576aea90-0564-437e-9099-146a244551d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28925 25561\n",
      "43967 10907\n"
     ]
    }
   ],
   "source": [
    "parsed_queries_path = \"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_aurora/aurora_mixed_parsed_queries.json\"\n",
    "plans = load_json(parsed_queries_path, namespace=False)\n",
    "folder_name = \"mixed_aurora\"\n",
    "directory = f\"/Users/ziniuw/Desktop/research/Data/AWS_trace/{folder_name}/\"\n",
    "all_raw_trace, all_trace = load_trace_all_version(directory, 8, concat=True)\n",
    "all_concurrency_df = []\n",
    "for trace in all_trace:\n",
    "    concurrency_df = create_concurrency_dataset(trace, engine=None, pre_exec_interval=200)\n",
    "    all_concurrency_df.append(concurrency_df)\n",
    "concurrency_df = pd.concat(all_concurrency_df, ignore_index=True)\n",
    "train_trace_df_sep, eval_trace_df_sep = pre_info_train_test_seperation(concurrency_df)\n",
    "print(len(train_trace_df_sep), len(eval_trace_df_sep))\n",
    "np.random.seed(0)\n",
    "train_idx = np.random.choice(len(concurrency_df), size=int(0.8 * len(concurrency_df)), replace=False)\n",
    "test_idx = [i for i in range(len(concurrency_df)) if i not in train_idx]\n",
    "train_trace_df = copy.deepcopy(concurrency_df.iloc[train_idx])\n",
    "eval_trace_df = concurrency_df.iloc[test_idx]\n",
    "eval_trace_df = copy.deepcopy(eval_trace_df[eval_trace_df['num_concurrent_queries'] > 0])\n",
    "print(len(train_trace_df), len(eval_trace_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10927c71-72be-4c2d-a154-939c6d129781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concurrency_df = create_concurrency_dataset(all_trace[0], engine=None, pre_exec_interval=200)\n",
    "len(concurrency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a4c039-d2b7-4322-91c8-6fc0b47ad549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 operators contains 0.9650782102582758 total operators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [00:01<00:00, 160.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% absolute error is 0.7304885387420654, q-error is 1.622496247291565\n",
      "90% absolute error is 7.360652923583984, q-error is 4.764204025268555\n",
      "95% absolute error is 17.918014526367188, q-error is 7.269602298736572\n"
     ]
    }
   ],
   "source": [
    "ss = SingleStage(use_table_features=True, true_card=True)\n",
    "#df = ss.featurize_data(train_trace_df, parsed_queries_path)\n",
    "df = ss.featurize_data(concurrency_df, parsed_queries_path)\n",
    "ss.train(df)\n",
    "rnn = ConcurrentRNN(ss, \n",
    "                    input_size=len(ss.all_feature[0]) * 2 + 7,\n",
    "                    embedding_dim=128,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=2,\n",
    "                    loss_function=\"q_loss\",\n",
    "                    last_output=True,\n",
    "                    use_seperation=False\n",
    "                   )\n",
    "rnn.load_model(\"checkpoints\")\n",
    "preds, labels = rnn.predict(eval_trace_df_sep, use_pre_info_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd91904f-26bd-4185-8c7c-6e696078cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>query_idx</th>\n",
       "      <th>runtime</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>pre_exec_info</th>\n",
       "      <th>concur_info</th>\n",
       "      <th>num_concurrent_queries</th>\n",
       "      <th>concur_info_train</th>\n",
       "      <th>num_concurrent_queries_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>110.210543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.210543</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(143, 0.0, 3.958360195159912), (135, 0.887088...</td>\n",
       "      <td>24</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>3.958360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.958360</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (135, 0.88708800...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(29, 0.0, 110.210542678833)]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3.479030</td>\n",
       "      <td>0.887088</td>\n",
       "      <td>4.366118</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (143, 0.0, 3.958...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (143, 0.0, 3.958...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>63.228388</td>\n",
       "      <td>6.449771</td>\n",
       "      <td>69.678159</td>\n",
       "      <td>[(143, 0.0, 3.958360195159912), (135, 0.887088...</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (36, 6.515793, 7...</td>\n",
       "      <td>9</td>\n",
       "      <td>[(29, 0.0, 110.210542678833)]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0.853417</td>\n",
       "      <td>6.515793</td>\n",
       "      <td>7.369210</td>\n",
       "      <td>[(143, 0.0, 3.958360195159912), (135, 0.887088...</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (75, 6.449771, 6...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (75, 6.449771, 6...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  query_idx     runtime  start_time    end_time  \\\n",
       "0      0         29  110.210543    0.000000  110.210543   \n",
       "1      1        143    3.958360    0.000000    3.958360   \n",
       "2      2        135    3.479030    0.887088    4.366118   \n",
       "3      3         75   63.228388    6.449771   69.678159   \n",
       "4      4         36    0.853417    6.515793    7.369210   \n",
       "\n",
       "                                       pre_exec_info  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [(143, 0.0, 3.958360195159912), (135, 0.887088...   \n",
       "4  [(143, 0.0, 3.958360195159912), (135, 0.887088...   \n",
       "\n",
       "                                         concur_info  num_concurrent_queries  \\\n",
       "0  [(143, 0.0, 3.958360195159912), (135, 0.887088...                      24   \n",
       "1  [(29, 0.0, 110.210542678833), (135, 0.88708800...                       2   \n",
       "2  [(29, 0.0, 110.210542678833), (143, 0.0, 3.958...                       2   \n",
       "3  [(29, 0.0, 110.210542678833), (36, 6.515793, 7...                       9   \n",
       "4  [(29, 0.0, 110.210542678833), (75, 6.449771, 6...                       2   \n",
       "\n",
       "                                   concur_info_train  \\\n",
       "0                                                 []   \n",
       "1                      [(29, 0.0, 110.210542678833)]   \n",
       "2  [(29, 0.0, 110.210542678833), (143, 0.0, 3.958...   \n",
       "3                      [(29, 0.0, 110.210542678833)]   \n",
       "4  [(29, 0.0, 110.210542678833), (75, 6.449771, 6...   \n",
       "\n",
       "   num_concurrent_queries_train  \n",
       "0                             0  \n",
       "1                             1  \n",
       "2                             2  \n",
       "3                             1  \n",
       "4                             2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concurrency_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8ff7d10-9717-4769-9b25-9c18fd55bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:00<00:00, 64.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% absolute error is 1.7421255111694336, q-error is 1.2857015132904053\n",
      "90% absolute error is 20.836120605468754, q-error is 3.267798900604248\n",
      "95% absolute error is 38.862800598144496, q-error is 4.820914173126212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scheduler = BaseScheduler(ss, rnn)\n",
    "simulator = Simulator(scheduler)\n",
    "concurrency_df = concurrency_df.sort_values(by=['start_time'], ascending=True)\n",
    "original_predictions = scheduler.make_original_prediction(concurrency_df)\n",
    "assert len(concurrency_df) == len(original_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12125423-e7a5-4f02-b102-83afc8980d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current time:  0.0\n",
      "running_queries:  [(0, 4.87758731842041)]\n",
      "queued_queries:  []\n",
      "current time:  0.001\n",
      "running_queries:  [(0, 6.818284), (1, 3.0237453)]\n",
      "queued_queries:  []\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     next_query_start_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m simulator\u001b[38;5;241m.\u001b[39mreplay_one_query(all_start_time[i], next_query_start_time, i, all_query_idx[i])\n\u001b[1;32m     12\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mprint_state()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m20\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mSimulator.replay_one_query\u001b[0;34m(self, start_time, next_query_start_time, query_str, query_idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplay_one_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_time: \u001b[38;5;28mfloat\u001b[39m, next_query_start_time: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m                      query_str: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, query_idx: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Todo: this logical should go to the scheduler\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     should_immediate_re_ingest, should_pause_and_re_ingest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mingest_query_simulation(\n\u001b[1;32m     51\u001b[0m         start_time, query_str\u001b[38;5;241m=\u001b[39mquery_str, query_idx\u001b[38;5;241m=\u001b[39mquery_idx\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_immediate_re_ingest:\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m# the scheduler schedules one query at a time even if there are multiple queries in the queue, so need to call again\u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_one_query(start_time \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 142\u001b[0m, in \u001b[0;36mBaseScheduler.ingest_query_simulation\u001b[0;34m(self, start_t, query_str, query_idx)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueued_query_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# nothing to do when there is no query in the queue\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m should_immediate_re_ingest, should_pause_and_re_ingest\n\u001b[0;32m--> 142\u001b[0m predictions, global_x, global_pre_info_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39monline_inference(\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting_query_features,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting_query_concur_features,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting_pre_info_length,\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueued_query_features,\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting_start_time,\n\u001b[1;32m    148\u001b[0m     start_t,\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Todo: add algorithms to decide whether to put in queue or directly for execution\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/research/perf_prediction/exploration/debug/../models/concurrency/complex_models.py:312\u001b[0m, in \u001b[0;36mConcurrentRNN.online_inference\u001b[0;34m(self, existing_query_features, existing_query_concur_features, existing_pre_info_length, queued_query_features, existing_start_time, current_time)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21monline_inference\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    305\u001b[0m     existing_query_features: List[np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     current_time: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    311\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, List[torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 312\u001b[0m     global_x, global_pre_info_length \u001b[38;5;241m=\u001b[39m featurize_queries_complex_online(\n\u001b[1;32m    313\u001b[0m         existing_query_features,\n\u001b[1;32m    314\u001b[0m         existing_query_concur_features,\n\u001b[1;32m    315\u001b[0m         existing_pre_info_length,\n\u001b[1;32m    316\u001b[0m         queued_query_features,\n\u001b[1;32m    317\u001b[0m         existing_start_time,\n\u001b[1;32m    318\u001b[0m         current_time,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    320\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(global_x, \u001b[38;5;28;01mNone\u001b[39;00m, global_pre_info_length, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions, global_x, global_pre_info_length\n",
      "File \u001b[0;32m~/Desktop/research/perf_prediction/exploration/debug/../models/feature/complex_rnn_features.py:110\u001b[0m, in \u001b[0;36mfeaturize_queries_complex_online\u001b[0;34m(existing_query_features, existing_query_concur_features, existing_pre_info_length, queued_query_features, existing_start_time, current_time)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m             x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(existing_query_concur_features[i])\n\u001b[0;32m--> 110\u001b[0m             x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, concur_query_feature), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    111\u001b[0m         global_x\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    112\u001b[0m global_pre_info_length \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(global_pre_info_length)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "\n",
    "original_runtime = []\n",
    "all_start_time = concurrency_df[\"start_time\"].values\n",
    "all_query_idx = concurrency_df[\"query_idx\"].values\n",
    "for i in range(len(concurrency_df)):\n",
    "    original_runtime.append(original_predictions[i])\n",
    "    # replaying the query one-by-one\n",
    "    if i < len(concurrency_df):\n",
    "        next_query_start_time = all_start_time[i + 1]\n",
    "    else:\n",
    "        next_query_start_time = None\n",
    "    simulator.replay_one_query(all_start_time[i], next_query_start_time, i, all_query_idx[i])\n",
    "    print(\"==============================\", i)\n",
    "    scheduler.print_state()\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a581d9-893a-4165-adde-b060047441e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.replay_one_query(1.87, 2.76, 143, 143)\n",
    "scheduler.print_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
