{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1ab8f9-2947-44fc-9de6-bd741c27eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "from parser.utils import load_json, dfs_cardinality, estimate_scan_in_mb\n",
    "from models.feature.single_xgboost_feature import find_top_k_operators, featurize_one_plan, get_top_k_table_by_size\n",
    "from utils.load_brad_trace import load_trace, create_concurrency_dataset, load_trace_all_version\n",
    "from models.concurrency.utils import pre_info_train_test_seperation\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from models.single.stage import SingleStage\n",
    "from models.concurrency.complex_models import ConcurrentRNN\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcb1507-a2fc-4323-a350-9566aee3b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "from utils.load_brad_trace import (\n",
    "    load_trace,\n",
    "    create_concurrency_dataset,\n",
    "    load_trace_all_version,\n",
    ")\n",
    "from models.single.stage import SingleStage\n",
    "from models.concurrency.complex_models import ConcurrentRNN\n",
    "from scheduler.base_scheduler import BaseScheduler\n",
    "\n",
    "\n",
    "class QueryBank:\n",
    "    def __init__(\n",
    "        self, sql_query_file: str, query_runtime_path: str, seed: int = 0\n",
    "    ) -> None:\n",
    "        with open(sql_query_file, \"r\") as f:\n",
    "            sql_queries = f.readlines()\n",
    "        query_runtime = np.load(query_runtime_path)\n",
    "        assert len(sql_queries) == len(query_runtime)\n",
    "        idx = np.argsort(query_runtime)\n",
    "        self.query_runtime = query_runtime[idx]\n",
    "        self.sql_queries = [sql_queries[i] for i in idx]\n",
    "        self.query_len = len(self.query_runtime)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def random_sample(self) -> (str, float):\n",
    "        # make a random sample of the query\n",
    "        idx = np.random.randint(self.query_len)\n",
    "        return self.sql_queries[idx], self.query_runtime[idx]\n",
    "\n",
    "    def sample_by_runtime(self, runtime: float) -> (str, float):\n",
    "        # sample a query that best matches the runtime\n",
    "        idx = np.searchsorted(self.query_runtime, runtime)\n",
    "        idx = max(idx, self.query_len - 1)\n",
    "        return self.sql_queries[idx], self.query_runtime[idx]\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(\n",
    "        self, scheduler: BaseScheduler, query_bank: Optional[QueryBank] = None, pause_wait_s: float = 5.0\n",
    "    ):\n",
    "        self.scheduler = scheduler\n",
    "        self.query_bank = query_bank\n",
    "        self.pause_wait_s = pause_wait_s\n",
    "\n",
    "    def replay_one_query(self, start_time: float, next_query_start_time: Optional[float] = None,\n",
    "                         query_str: Optional[int] = None, query_idx: Optional[int] = None):\n",
    "        # Todo: this logical should go to the scheduler\n",
    "        should_immediate_re_ingest, should_pause_and_re_ingest, scheduled_submit = self.scheduler.ingest_query_simulation(\n",
    "            start_time, query_str=query_str, query_idx=query_idx\n",
    "        )\n",
    "        if should_immediate_re_ingest:\n",
    "            # the scheduler schedules one query at a time even if there are multiple queries in the queue, so need to call again\n",
    "            self.replay_one_query(start_time + 0.001, next_query_start_time)\n",
    "        if should_pause_and_re_ingest:\n",
    "            if next_query_start_time is not None and next_query_start_time <= start_time + self.pause_wait_s:\n",
    "                return\n",
    "            self.replay_one_query(start_time + self.pause_wait_s, next_query_start_time)\n",
    "\n",
    "    def replay_workload(self, directory: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        all_raw_trace, all_trace = load_trace(directory, 8, concat=True)\n",
    "        concurrency_df = create_concurrency_dataset(all_trace, engine=None, pre_exec_interval=200)\n",
    "        concurrency_df = concurrency_df.sort_values(by=['start_time'], ascending=True)\n",
    "        original_predictions = self.scheduler.make_original_prediction(concurrency_df)\n",
    "        assert len(concurrency_df) == len(original_predictions)\n",
    "        original_runtime = []\n",
    "        all_start_time = concurrency_df[\"start_time\"].values\n",
    "        all_query_idx = concurrency_df[\"query_idx\"].values\n",
    "        for i in range(len(concurrency_df)):\n",
    "            original_runtime.append(original_predictions[i])\n",
    "            # replaying the query one-by-one\n",
    "            if i < len(concurrency_df):\n",
    "                next_query_start_time = all_start_time[i + 1]\n",
    "            else:\n",
    "                next_query_start_time = None\n",
    "            self.replay_one_query(all_start_time[i], next_query_start_time, i, all_query_idx[i])\n",
    "        # finish all queries\n",
    "        self.scheduler.finish_query(np.infty)\n",
    "        new_runtime = []\n",
    "        for i in range(len(concurrency_df)):\n",
    "            new_runtime.append(self.scheduler.all_query_runtime[i])\n",
    "        original_runtime = np.asarray(original_runtime)\n",
    "        new_runtime = np.asarray(new_runtime)\n",
    "        return original_runtime, new_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ce284a-0421-456f-b645-8acd9cf01d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Tuple, List, Union, MutableMapping\n",
    "from models.single.stage import SingleStage\n",
    "from models.concurrency.complex_models import ConcurrentRNN\n",
    "from scheduler.base_scheduler import BaseScheduler\n",
    "\n",
    "\n",
    "class GreedyScheduler(BaseScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stage_model: SingleStage,\n",
    "        predictor: ConcurrentRNN,\n",
    "        max_concurrency_level: int = 10,\n",
    "        min_concurrency_level: int = 2,\n",
    "    ):\n",
    "        super(GreedyScheduler, self).__init__(\n",
    "            stage_model, predictor, max_concurrency_level, min_concurrency_level\n",
    "        )\n",
    "\n",
    "    def ingest_query_simulation(\n",
    "        self,\n",
    "        start_t: float,\n",
    "        query_str: Optional[Union[str, int]] = None,\n",
    "        query_idx: Optional[int] = None,\n",
    "    ) -> Tuple[bool, bool, Optional[float]]:\n",
    "        \"\"\"We work on planning the currently queued queries if query_str is None (i.e., no query submitted)\"\"\"\n",
    "        self.current_time = start_t\n",
    "        self.finish_query()\n",
    "        should_immediate_re_ingest = False\n",
    "        should_pause_and_re_ingest = False\n",
    "        scheduled_submit = None\n",
    "        if query_str is not None:\n",
    "            self.queued_queries.append(query_str)\n",
    "            self.queued_queries_enter_time.append(start_t)\n",
    "            query_feature = self.stage_model.featurize_online(query_idx)\n",
    "            self.queued_query_features.append(query_feature)\n",
    "\n",
    "        if len(self.queued_query_features) == 0:\n",
    "            # nothing to do when there is no query in the queue\n",
    "            return (\n",
    "                should_immediate_re_ingest,\n",
    "                should_pause_and_re_ingest,\n",
    "                scheduled_submit,\n",
    "            )\n",
    "        if len(self.existing_finish_time) == 0:\n",
    "            next_finish_idx = None\n",
    "            next_finish_time = None\n",
    "        else:\n",
    "            next_finish_idx = np.argmin(self.existing_finish_time)\n",
    "            next_finish_time = self.existing_finish_time[next_finish_idx]\n",
    "\n",
    "        predictions, global_x, global_pre_info_length = self.predictor.online_inference(\n",
    "            self.existing_query_features,\n",
    "            self.existing_query_concur_features,\n",
    "            self.existing_pre_info_length,\n",
    "            self.queued_query_features,\n",
    "            self.existing_start_time,\n",
    "            start_t,\n",
    "            next_finish_idx=next_finish_idx,\n",
    "            next_finish_time=next_finish_time,\n",
    "            get_next_finish=True\n",
    "        )\n",
    "\n",
    "        predictions = predictions.reshape(-1).detach().numpy()\n",
    "        # Todo: add algorithms to decide whether to put in queue or directly for execution\n",
    "        if len(self.running_queries) == 0:\n",
    "            # submit the shortest running query in queue when there is no query running\n",
    "            # Todo: this is not optimal\n",
    "            assert len(predictions) == 2 * len(self.queued_queries)\n",
    "\n",
    "            predictions_query = predictions[0:-1:2]\n",
    "            selected_idx = np.argmin(predictions_query)\n",
    "            self.submit_query(\n",
    "                selected_idx,\n",
    "                self.queued_queries[selected_idx],\n",
    "                predictions_query[selected_idx],\n",
    "                self.queued_query_features[selected_idx],\n",
    "                start_t,\n",
    "                self.queued_queries_enter_time[selected_idx],\n",
    "                float(predictions_query[selected_idx]) + start_t,\n",
    "                None,\n",
    "                int(global_pre_info_length[selected_idx * 2]),\n",
    "            )\n",
    "            should_immediate_re_ingest = True\n",
    "            return (\n",
    "                should_immediate_re_ingest,\n",
    "                should_pause_and_re_ingest,\n",
    "                scheduled_submit,\n",
    "            )\n",
    "        elif len(self.running_queries) >= self.max_concurrency_level:\n",
    "            # when the system is overloaded, should pause and retry\n",
    "            should_pause_and_re_ingest = True\n",
    "            return (\n",
    "                should_immediate_re_ingest,\n",
    "                should_pause_and_re_ingest,\n",
    "                scheduled_submit,\n",
    "            )\n",
    "        else:\n",
    "            all_score = []\n",
    "            all_query_idx = []\n",
    "            for i in range(len(self.queued_queries)):\n",
    "                pred_idx = i * (2 + len(self.existing_query_concur_features))\n",
    "                curr_pred = predictions[pred_idx]\n",
    "                submit_after_pred = predictions[pred_idx + 1]\n",
    "                # how does the predicted runtime of submitting now compare to submitting later\n",
    "                curr_delta = curr_pred - submit_after_pred + (next_finish_time - start_t)\n",
    "                old_existing_pred = np.asarray(self.existing_runtime_prediction)\n",
    "                new_existing_pred = predictions[\n",
    "                                    (pred_idx + 2): (\n",
    "                                            pred_idx + len(self.existing_query_concur_features) + 2\n",
    "                                    )\n",
    "                                    ]\n",
    "                # how will this query change the runtime of existing queries in the system ()\n",
    "                delta = new_existing_pred - old_existing_pred\n",
    "                delta_sum = np.sum(delta)\n",
    "                #print(self.queued_queries[i], curr_delta, delta_sum, delta)\n",
    "                # for every query first judge whether it is good to wait\n",
    "                if curr_delta + delta_sum < 0:\n",
    "                    # when the current system state benefit the current query more than\n",
    "                    # this query's (probably negative) impact on the running queries\n",
    "                    # more optimal to submit now than later\n",
    "                    all_score.append(delta_sum + curr_delta)\n",
    "                    all_query_idx.append(i)\n",
    "                    # todo: add more clever conditions\n",
    "            if len(all_score) == 0:\n",
    "                should_immediate_re_ingest = False\n",
    "                should_pause_and_re_ingest = False\n",
    "                # Todo implement scheduled submit in the future\n",
    "                # now we just pause and wait for re_ingest\n",
    "                scheduled_submit = None\n",
    "            else:\n",
    "                best_query_idx = np.argmin(all_score)\n",
    "                selected_idx = all_query_idx[best_query_idx]\n",
    "                converted_idx = selected_idx * (\n",
    "                    2 + len(self.existing_query_concur_features)\n",
    "                )\n",
    "                curr_pred_runtime = predictions[converted_idx]\n",
    "                finish_t = start_t + curr_pred_runtime\n",
    "                existing_query_concur_features = global_x[converted_idx]\n",
    "                new_existing_pred = predictions[\n",
    "                    (converted_idx + 2) : (\n",
    "                        converted_idx + len(self.existing_query_concur_features) + 2\n",
    "                    )\n",
    "                ]\n",
    "                new_existing_finish_time = []\n",
    "                for i in range(len(self.existing_start_time)):\n",
    "                    new_existing_finish_time.append(\n",
    "                        new_existing_pred[i] + self.existing_start_time[i]\n",
    "                    )\n",
    "                new_existing_query_concur_feature = global_x[\n",
    "                    (converted_idx + 2) : (\n",
    "                        converted_idx + len(self.existing_query_concur_features) + 2\n",
    "                    )\n",
    "                ]\n",
    "                self.submit_query(\n",
    "                    selected_idx,\n",
    "                    self.queued_queries[selected_idx],\n",
    "                    curr_pred_runtime,\n",
    "                    self.queued_query_features[selected_idx],\n",
    "                    start_t,\n",
    "                    self.queued_queries_enter_time[selected_idx],\n",
    "                    finish_t,\n",
    "                    existing_query_concur_features,\n",
    "                    int(global_pre_info_length[converted_idx]),\n",
    "                    new_existing_finish_time,\n",
    "                    list(new_existing_pred),\n",
    "                    new_existing_query_concur_feature,\n",
    "                )\n",
    "                should_immediate_re_ingest = True\n",
    "                should_pause_and_re_ingest = False\n",
    "                scheduled_submit = None\n",
    "            return (\n",
    "                should_immediate_re_ingest,\n",
    "                should_pause_and_re_ingest,\n",
    "                scheduled_submit,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576aea90-0564-437e-9099-146a244551d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28925 25561\n",
      "43967 10907\n"
     ]
    }
   ],
   "source": [
    "parsed_queries_path = \"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_aurora/aurora_mixed_parsed_queries.json\"\n",
    "plans = load_json(parsed_queries_path, namespace=False)\n",
    "folder_name = \"mixed_aurora\"\n",
    "directory = f\"/Users/ziniuw/Desktop/research/Data/AWS_trace/{folder_name}/\"\n",
    "all_raw_trace, all_trace = load_trace_all_version(directory, 8, concat=True)\n",
    "all_concurrency_df = []\n",
    "for trace in all_trace:\n",
    "    concurrency_df = create_concurrency_dataset(trace, engine=None, pre_exec_interval=200)\n",
    "    all_concurrency_df.append(concurrency_df)\n",
    "concurrency_df = pd.concat(all_concurrency_df, ignore_index=True)\n",
    "train_trace_df_sep, eval_trace_df_sep = pre_info_train_test_seperation(concurrency_df)\n",
    "print(len(train_trace_df_sep), len(eval_trace_df_sep))\n",
    "np.random.seed(0)\n",
    "train_idx = np.random.choice(len(concurrency_df), size=int(0.8 * len(concurrency_df)), replace=False)\n",
    "test_idx = [i for i in range(len(concurrency_df)) if i not in train_idx]\n",
    "train_trace_df = copy.deepcopy(concurrency_df.iloc[train_idx])\n",
    "eval_trace_df = concurrency_df.iloc[test_idx]\n",
    "eval_trace_df = copy.deepcopy(eval_trace_df[eval_trace_df['num_concurrent_queries'] > 0])\n",
    "print(len(train_trace_df), len(eval_trace_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10927c71-72be-4c2d-a154-939c6d129781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concurrency_df = create_concurrency_dataset(all_trace[4], engine=None, pre_exec_interval=200)\n",
    "len(concurrency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a4c039-d2b7-4322-91c8-6fc0b47ad549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 operators contains 0.9650782102582758 total operators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 162.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% absolute error is 0.9629056453704834, q-error is 1.7662272453308105\n",
      "90% absolute error is 8.765890121459961, q-error is 6.40096378326416\n",
      "95% absolute error is 19.13550567626953, q-error is 10.512479782104492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ss = SingleStage(use_table_features=True, true_card=False, use_median=True)\n",
    "#df = ss.featurize_data(train_trace_df, parsed_queries_path)\n",
    "df = ss.featurize_data(concurrency_df, parsed_queries_path)\n",
    "ss.train(df)\n",
    "rnn = ConcurrentRNN(ss, \n",
    "                    input_size=len(ss.all_feature[0]) * 2 + 7,\n",
    "                    embedding_dim=128,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=2,\n",
    "                    loss_function=\"q_loss\",\n",
    "                    last_output=True,\n",
    "                    use_seperation=False\n",
    "                   )\n",
    "rnn.load_model(\"checkpoints\")\n",
    "preds, labels = rnn.predict(eval_trace_df_sep, use_pre_info_only=False, return_per_query=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8ff7d10-9717-4769-9b25-9c18fd55bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 57/57 [00:01<00:00, 52.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50% absolute error is 2.369178056716919, q-error is 1.2800177335739136\n",
      "90% absolute error is 27.301742172241227, q-error is 3.465474414825442\n",
      "95% absolute error is 53.84303741455077, q-error is 5.859610033035272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scheduler = GreedyScheduler(ss, rnn)\n",
    "simulator = Simulator(scheduler)\n",
    "concurrency_df = concurrency_df.sort_values(by=['start_time'], ascending=True)\n",
    "original_predictions = scheduler.make_original_prediction(concurrency_df)\n",
    "assert len(concurrency_df) == len(original_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12125423-e7a5-4f02-b102-83afc8980d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_runtime = []\n",
    "all_start_time = concurrency_df[\"start_time\"].values\n",
    "all_query_idx = concurrency_df[\"query_idx\"].values\n",
    "for i in range(len(concurrency_df)):\n",
    "    original_runtime.append(original_predictions[i])\n",
    "    # replaying the query one-by-one\n",
    "    if i < len(concurrency_df) - 1:\n",
    "        next_query_start_time = all_start_time[i + 1]\n",
    "    else:\n",
    "        next_query_start_time = None\n",
    "    #print(\"==============================\", i, original_predictions[i])\n",
    "    simulator.replay_one_query(all_start_time[i], next_query_start_time, i, all_query_idx[i])\n",
    "    #scheduler.print_state()\n",
    "simulator.scheduler.finish_query(np.infty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0b5811-ba5b-4059-bdd9-5bfb3cfaff62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "7242",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m new_runtime \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(concurrency_df)):\n\u001b[0;32m----> 3\u001b[0m     new_runtime\u001b[38;5;241m.\u001b[39mappend(simulator\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mall_query_runtime[i])\n\u001b[1;32m      4\u001b[0m original_runtime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(original_runtime)\n\u001b[1;32m      5\u001b[0m new_runtime \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(new_runtime)\n",
      "\u001b[0;31mKeyError\u001b[0m: 7242"
     ]
    }
   ],
   "source": [
    "new_runtime = []\n",
    "for i in range(len(concurrency_df)):\n",
    "    new_runtime.append(simulator.scheduler.all_query_runtime[i])\n",
    "original_runtime = np.asarray(original_runtime)\n",
    "new_runtime = np.asarray(new_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75517bf4-addf-485d-b6de-13854df244a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [7242, 7246])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.running_queries, scheduler.queued_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79ad93da-b2ca-4b31-8400-f233e329cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7246] [7242]\n",
      "[7242] []\n"
     ]
    }
   ],
   "source": [
    "st = all_start_time[-1]\n",
    "while len(scheduler.queued_queries) != 0:\n",
    "    simulator.replay_one_query(st + 3, None)\n",
    "    print(scheduler.running_queries, scheduler.queued_queries)\n",
    "    st += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8e45aea-7c92-4086-8b95-89575bf0225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_r = original_runtime[:7242]\n",
    "n_r = new_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed31f68c-522b-4d2f-a283-bedaac085484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3403128827096682"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "742ce786-10c1-42bc-a8e5-bcc5e7cc3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = concurrency_df['runtime'].values[:7242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "454d6d94-b0e0-4dd3-8824-7d5c89fa65f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50.01323,\n",
       " 10.085129737854004,\n",
       " 150.9403045654297,\n",
       " 239.2269371032714,\n",
       " 487.9498553466801)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(o_r), np.percentile(o_r, 50), np.percentile(o_r, 90), np.percentile(o_r, 95), np.percentile(o_r, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef981923-f314-4c1d-a8a0-7e665839619f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.593487873728154,\n",
       " 5.124333381652832,\n",
       " 42.939343800512546,\n",
       " 73.74249505377956,\n",
       " 189.0873755398267)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(n_r), np.percentile(n_r, 50), np.percentile(n_r, 90), np.percentile(n_r, 95), np.percentile(n_r, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2aa09bd-d58e-4d59-904c-f1904b29bc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58.2194605004027,\n",
       " 10.538796186447144,\n",
       " 171.31692030429855,\n",
       " 280.9423947334289,\n",
       " 579.0570925283433)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rt), np.percentile(rt, 50), np.percentile(rt, 90), np.percentile(rt, 95), np.percentile(rt, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a581d9-893a-4165-adde-b060047441e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.replay_one_query(1.87, 2.76, 143, 143)\n",
    "scheduler.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c2093d9-5338-44f1-8fcd-cd7075b47414",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142c01d3-6b6e-47b6-ab99-3b92811d6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a680d3a-30d6-4d00-aeb3-64e171e22506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dd7b1-c0c2-4e0d-abe2-ca984c65626a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
