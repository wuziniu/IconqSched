{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c22c477-d91e-4105-8faf-4f47a8305469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.load_brad_trace import load_trace, create_concurrency_dataset, load_trace_all_version\n",
    "from models.concurrency.analytical_models import SimpleFitCurve, ComplexFitCurve\n",
    "from models.concurrency.xgboost import XGBoostPredictor\n",
    "from models.concurrency.linear_regression import SimpleLinearReg\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06226fd1-03ad-4fb5-ba19-11108c4be863",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"mixed_aurora\"\n",
    "directory = f\"/Users/ziniuw/Desktop/research/Data/AWS_trace/{folder_name}/\"\n",
    "all_raw_trace, all_trace = load_trace_all_version(directory, 8, concat=True)\n",
    "all_concurrency_df = []\n",
    "for trace in all_trace:\n",
    "    concurrency_df = create_concurrency_dataset(trace, engine=None, pre_exec_interval=400)\n",
    "    all_concurrency_df.append(concurrency_df)\n",
    "concurrency_df = pd.concat(all_concurrency_df, ignore_index=True)\n",
    "isolated_trace_df = pd.read_csv(f\"/Users/ziniuw/Desktop/research/Data/AWS_trace/{folder_name}/repeating_olap_batch_warmup.csv\")\n",
    "#isolated_trace_df = pd.read_csv(f\"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_redshift/repeating_olap_batch_warmup.csv\")\n",
    "isolated_trace_df[\"runtime\"] = isolated_trace_df[\"run_time_s\"]\n",
    "isolated_rt_cache = dict()\n",
    "for i, rows in isolated_trace_df.groupby(\"query_idx\"):\n",
    "    isolated_rt_cache[i] = np.median(rows[\"runtime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f3ca054-c27b-4bad-8946-984c8738fc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>query_idx</th>\n",
       "      <th>runtime</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>pre_exec_info</th>\n",
       "      <th>concur_info</th>\n",
       "      <th>num_concurrent_queries</th>\n",
       "      <th>concur_info_train</th>\n",
       "      <th>num_concurrent_queries_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>110.210543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.210543</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(143, 0.0, 3.958360195159912), (135, 0.887088...</td>\n",
       "      <td>24</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>3.958360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.958360</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (135, 0.88708800...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(29, 0.0, 110.210542678833)]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>3.479030</td>\n",
       "      <td>0.887088</td>\n",
       "      <td>4.366118</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (143, 0.0, 3.958...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (143, 0.0, 3.958...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>63.228388</td>\n",
       "      <td>6.449771</td>\n",
       "      <td>69.678159</td>\n",
       "      <td>[(143, 0.0, 3.958360195159912), (135, 0.887088...</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (36, 6.515793, 7...</td>\n",
       "      <td>9</td>\n",
       "      <td>[(29, 0.0, 110.210542678833)]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0.853417</td>\n",
       "      <td>6.515793</td>\n",
       "      <td>7.369210</td>\n",
       "      <td>[(143, 0.0, 3.958360195159912), (135, 0.887088...</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (75, 6.449771, 6...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(29, 0.0, 110.210542678833), (75, 6.449771, 6...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  query_idx     runtime  start_time    end_time  \\\n",
       "0      0         29  110.210543    0.000000  110.210543   \n",
       "1      1        143    3.958360    0.000000    3.958360   \n",
       "2      2        135    3.479030    0.887088    4.366118   \n",
       "3      3         75   63.228388    6.449771   69.678159   \n",
       "4      4         36    0.853417    6.515793    7.369210   \n",
       "\n",
       "                                       pre_exec_info  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3  [(143, 0.0, 3.958360195159912), (135, 0.887088...   \n",
       "4  [(143, 0.0, 3.958360195159912), (135, 0.887088...   \n",
       "\n",
       "                                         concur_info  num_concurrent_queries  \\\n",
       "0  [(143, 0.0, 3.958360195159912), (135, 0.887088...                      24   \n",
       "1  [(29, 0.0, 110.210542678833), (135, 0.88708800...                       2   \n",
       "2  [(29, 0.0, 110.210542678833), (143, 0.0, 3.958...                       2   \n",
       "3  [(29, 0.0, 110.210542678833), (36, 6.515793, 7...                       9   \n",
       "4  [(29, 0.0, 110.210542678833), (75, 6.449771, 6...                       2   \n",
       "\n",
       "                                   concur_info_train  \\\n",
       "0                                                 []   \n",
       "1                      [(29, 0.0, 110.210542678833)]   \n",
       "2  [(29, 0.0, 110.210542678833), (143, 0.0, 3.958...   \n",
       "3                      [(29, 0.0, 110.210542678833)]   \n",
       "4  [(29, 0.0, 110.210542678833), (75, 6.449771, 6...   \n",
       "\n",
       "   num_concurrent_queries_train  \n",
       "0                             0  \n",
       "1                             1  \n",
       "2                             2  \n",
       "3                             1  \n",
       "4                             2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concurrency_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d41f999-2e9a-4545-a17d-2d486cd4e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 229 44.55030298233032 149.43657204573853 3.882469892501831 928.3928072452544 162.89973109444279\n",
      "1 230 0.2543864250183105 0.6743606847265492 0.0791635513305664 7.855563879013061 1.0985220381229965\n",
      "2 226 23.157404899597168 152.3746449219442 6.902190923690796 949.1584985256196 156.9762234685832\n",
      "3 231 0.283794641494751 0.40228485235404143 0.0791168212890625 10.761748790740969 0.8148074521967292\n",
      "4 228 4.330385208129883 21.139591959484836 0.0939805507659912 173.76991820335388 28.61461356127559\n",
      "5 233 2.5214909315109253 17.549908976698127 2.4990737438201904 248.4405851364136 33.45214806905375\n",
      "6 229 2.5594241619110107 3.7198088002517236 2.478666305541992 18.826606512069706 2.124091232234117\n",
      "7 230 2.2192370891571045 10.955201000752657 1.7622826099395752 173.82364749908447 23.275037854071385\n",
      "8 232 0.3999779224395752 1.2264011348115986 0.1878495216369629 19.547641038894653 2.632632301241479\n",
      "9 232 0.7344051599502563 0.10832449588282352 0.0426087379455566 8.005128622055054 0.5284501040445094\n",
      "10 228 9.838077902793884 36.84041999620304 6.391626834869385 317.7200520038605 46.24118694973256\n",
      "11 229 17.317299365997314 83.7259368240573 14.00517749786377 328.1350717544556 68.09873422860949\n",
      "12 225 1.0532492399215698 7.105654069052802 0.7002675533294678 168.14855551719666 14.916034252433114\n",
      "13 231 0.917332649230957 3.0217302454498425 0.0850152969360351 54.99982976913452 6.181148552820167\n",
      "14 226 1.0710943937301636 6.211981946388177 0.1146163940429687 106.03912711143494 11.79848321999748\n",
      "15 230 1.1313714981079102 10.527817219236622 1.1345529556274414 163.53098440170288 21.529779224643484\n",
      "16 228 7.890487909317017 121.00611078843735 7.651093482971191 576.9549643993378 92.25077205990128\n",
      "17 231 34.81024467945099 199.30163099136186 11.320307731628418 926.4755551815032 167.43395683427795\n",
      "18 229 30.62745499610901 65.65551691180234 18.98758602142334 365.5455613136292 52.256663618580845\n",
      "19 232 0.23818504810333252 0.8857434391975403 0.2334463596343994 53.47558784484863 3.570511683011313\n",
      "20 227 0.7317588329315186 9.229599176524495 0.6916830539703369 122.2988142967224 16.330047368819244\n",
      "21 231 0.2451510429382324 0.7238011153745445 0.1922049522399902 11.117865562438965 1.2498752501658854\n",
      "22 215 90.76315653324127 272.26353419326074 4.003665447235107 919.2871561050416 214.56459478880436\n",
      "23 229 0.0922937393188476 0.17601883255238096 0.0805838108062744 1.0360658168792725 0.10152836233140117\n",
      "24 227 1.251265048980713 2.9298438437709726 0.1472306251525879 156.8522002696991 11.384857182103016\n",
      "25 228 1.8297585248947144 10.626836163955822 0.9000635147094727 157.5043966770172 21.958118617490992\n",
      "26 226 1.1069748401641846 2.075078276406347 0.6302692890167236 19.371718168258667 2.817728265653363\n",
      "27 229 46.07182168960571 220.15153552767492 5.610898733139038 903.6508729457856 192.990322614407\n",
      "28 232 17.439111709594727 62.80936398691144 1.0793287754058838 995.3244767189026 111.65625406482803\n",
      "29 228 56.18412530422211 211.95663373825843 6.82301139831543 944.2941653728484 193.65760365191124\n",
      "30 229 0.09448230266571045 0.22380701527324826 0.0887396335601806 1.4510889053344729 0.200292674567537\n",
      "31 228 2.4314125776290894 72.93485825731044 2.379435777664185 719.217169046402 118.96554558448203\n",
      "32 227 0.17349159717559812 0.7615802372079589 0.1020703315734863 32.255064725875854 2.430554988422137\n",
      "33 229 0.2653744220733642 5.296810422922326 0.0403573513031005 127.85139727592468 15.046714717195965\n",
      "34 228 34.28357934951782 144.29294518734278 16.993483543395996 656.171219587326 123.83146391256795\n",
      "35 229 0.0837587118148803 0.1991702510800424 0.0796742439270019 1.9662773609161377 0.19323079746293523\n",
      "36 230 0.07498061656951899 1.7426094075907832 0.0570065975189209 51.46196579933167 5.172352313223642\n",
      "37 229 1.9500285387039185 10.063717091447922 0.5775494575500488 133.75567150115967 22.212107972131594\n",
      "38 226 1.0670592784881592 13.881688981984569 0.6128857135772705 216.2384307384491 25.951665923281375\n",
      "39 228 4.3417723178863525 74.33488407051354 1.1878938674926758 719.0044183731079 89.81948440691131\n",
      "40 230 1.1273598670959473 9.479451046819273 1.127530574798584 142.07300806045532 20.815476424449727\n",
      "41 230 0.8049191236495972 6.149765000136003 0.1963379383087158 77.48524904251099 11.458730168774526\n",
      "42 233 0.531968355178833 1.4681738804338316 0.4279801845550537 28.78038215637207 2.323563768109597\n",
      "43 228 1.0895878076553345 7.063695701590755 0.5327754020690918 219.1338093280792 23.268904453711542\n",
      "44 231 0.31516551971435547 0.8806594357346044 0.3059141635894775 24.05018281936645 1.7736753538748375\n",
      "45 230 2.0758092403411865 21.475610414795252 2.004286289215088 175.16619157791138 31.287390735957576\n",
      "46 229 1.6134135723114014 69.84435836508806 1.599963903427124 371.619517326355 71.17105925854187\n",
      "47 233 0.6185528039932251 2.9270756991636087 0.6077203750610352 40.77281069755554 5.505956293879086\n",
      "48 233 2.655856966972351 72.66679109831225 2.631626605987549 390.5430369377136 74.17700713890763\n",
      "49 226 0.4902068376541137 59.04365409264523 0.422067642211914 743.5387005805969 104.23815632867935\n",
      "50 230 3.9616836309432983 15.580137386529342 3.829664468765259 99.64675402641296 19.968354271189742\n",
      "51 228 0.20404422283172607 0.47636366936198454 0.1943397521972656 4.95866847038269 0.5804857212171499\n",
      "52 228 0.1888930797576904 4.9712142892051165 0.0904526710510253 50.84340715408325 7.395874587494264\n",
      "53 231 0.04880917072296135 0.21816296494884407 0.0322492122650146 5.998992204666138 0.48205341130482027\n",
      "54 234 2.0857409238815308 5.058383729722765 2.039296865463257 65.34745073318481 6.902516943252495\n",
      "55 228 0.9463112354278564 5.472577833292777 0.305448055267334 101.03054237365724 12.097913295474807\n",
      "56 228 0.1158468723297119 0.34711429633592306 0.1122944355010986 8.602424621582031 0.7114837429112189\n",
      "57 232 0.0217760801315307 0.14056976088162126 0.0191121101379394 4.8354785442352295 0.3814485912575038\n",
      "58 228 11.741411089897156 119.4100841796189 7.98529052734375 662.1926944255829 109.6082360928541\n",
      "59 228 0.7437019348144531 1.928354140959288 0.7284054756164551 26.893789052963257 2.565075945066828\n",
      "60 228 0.7380635738372803 12.599463613409744 0.3196041584014892 270.21160888671875 25.674874654584443\n",
      "61 232 2.0613138675689697 29.536590781705133 0.772925615310669 347.4006507396698 41.43442971068805\n",
      "62 230 0.019817471504211398 0.9919038793315058 0.0189220905303955 11.26612401008606 1.7481525430545357\n",
      "63 228 0.11598217487335205 0.45367928659706785 0.1005241870880127 6.379046440124512 0.5960852741448489\n",
      "64 229 0.14782893657684326 0.694776042580084 0.129967451095581 6.898321866989136 1.0013299818210124\n",
      "65 230 15.957736253738403 118.8061783624732 16.747931241989136 500.6288983821869 87.8472386620448\n",
      "66 230 0.22978508472442621 2.8658062509868456 0.0492241382598876 66.47736287117004 7.006408947032644\n",
      "67 226 1.1211917400360107 10.81646497587187 1.114715814590454 147.4394290447235 19.91562192578941\n",
      "68 227 0.018006920814514098 0.02886995555020635 0.0168914794921875 0.2361721992492675 0.02191368503286629\n",
      "69 229 2.5562736988067627 71.09787870390447 2.520961284637451 666.1604583263397 84.64248516062958\n",
      "70 227 0.09705424308776855 0.4128172744213222 0.0975511074066162 5.79804253578186 0.6435190196616104\n",
      "71 232 0.0578186511993408 0.6740908211675184 0.0549912452697753 20.64253282546997 1.6133859140175923\n",
      "72 226 0.5103834867477417 2.0077989270201826 0.5058295726776123 98.4733271598816 7.21572822416859\n",
      "73 229 0.05470538139343255 0.41276944464471144 0.0442168712615966 8.570776462554932 0.9016929515828707\n",
      "74 232 4.543649435043335 91.18229739727646 4.2185235023498535 803.5779163837433 98.45735617801516\n",
      "75 231 15.195699691772461 138.3064436396479 17.57677125930786 627.7803311347961 126.29626199984767\n",
      "76 227 3.2767648696899414 9.626476952683033 0.4469208717346191 122.94531106948853 20.607349406166072\n",
      "77 204 90.21518075466156 404.9761011273253 52.51991677284241 971.3359701633452 222.95588863137445\n",
      "78 231 1.009168267250061 15.719068430202864 0.3781852722167969 138.85832262039185 23.779263908970194\n",
      "79 229 1.358502745628357 40.452383690005306 0.2827701568603515 310.8654396533966 55.88417738943685\n",
      "80 230 4.5670928955078125 27.727943628767264 4.613707065582275 195.95753002166748 36.54439259024802\n",
      "81 229 60.86928570270538 198.43093328809113 62.69926404953003 639.0156462192535 115.96865478921322\n",
      "82 230 1.6643879413604736 4.073957687875499 1.6695685386657717 73.83067297935486 7.219605657447397\n",
      "83 226 3.142974615097046 14.131048065371218 3.14095401763916 217.8042540550232 24.85991450818625\n",
      "84 236 1.9504293203353882 20.15597207566439 0.3958890438079834 235.22810316085813 33.94435145684253\n",
      "85 231 0.2782171964645386 0.4585090994318842 0.1149351596832275 6.001787662506104 0.5737302295590594\n",
      "86 230 1.0979831218719482 19.0818485767945 0.8396329879760742 276.8872349262237 36.98407949985245\n",
      "87 229 0.39867138862609863 1.2317437823682893 0.4032855033874511 18.61330986022949 2.0642625344945094\n",
      "88 228 0.09861612319946286 0.2325443845046194 0.0967535972595214 1.6769328117370603 0.2014953313104237\n",
      "89 227 0.9205673933029175 4.010458673149478 0.4741959571838379 81.23723196983337 7.553880235406229\n",
      "90 232 0.0456854104995727 0.1324423839306009 0.0374572277069091 2.1583914756774902 0.19095530677158687\n",
      "91 232 12.178162813186646 66.96740982450288 5.284567356109619 699.8655858039856 93.4562973612006\n",
      "92 231 0.8267189264297485 12.447826662104884 0.8206794261932373 104.83403515815736 22.16819523581478\n",
      "93 229 4.892933368682861 41.58343392480409 0.67997145652771 270.8483657836914 52.713757841239406\n",
      "94 232 4.9561803340911865 86.39500974170093 3.980847358703613 489.3032310009002 85.79912794436554\n",
      "95 230 0.14854192733764643 0.39255721983702285 0.1487677097320556 11.346787929534912 0.9329884313474309\n",
      "96 234 0.25272083282470703 0.4937317310235439 0.1894252300262451 19.585179328918457 1.3171186511199195\n",
      "97 231 0.10994160175323484 0.460704906678303 0.064112901687622 13.672101497650146 1.2627215324056131\n",
      "98 230 0.8793638944625854 12.196052690174268 0.766582727432251 139.58351159095764 20.874520745419492\n",
      "99 228 0.0927834510803222 0.5375134171101085 0.084583044052124 10.942650318145752 1.0861321235953176\n",
      "100 231 9.790285110473635 24.81538722628639 6.4061548709869385 229.9788167476654 31.205775006945885\n",
      "101 234 9.81118094921112 67.7207858114161 0.8592650890350342 762.5423293113708 101.37960920328733\n",
      "102 229 6.615021705627441 70.86993625278556 4.813796043395996 435.7608921527863 82.44546366840252\n",
      "103 118 84.98235750198364 682.0525401285139 113.45377659797668 981.9632034301758 249.52571213676848\n",
      "104 225 1.2356332540512085 20.65070220311483 1.2285277843475342 353.0752680301666 46.6206509845756\n",
      "105 235 1.410673975944519 17.06846762515129 1.2101259231567385 288.833594083786 35.63512573180825\n",
      "106 227 17.80790936946869 88.99082152748949 14.934360980987549 351.39323377609253 78.42830823412889\n",
      "107 231 28.366859316825867 133.67329347288452 6.133017301559448 990.1949307918547 140.12535353402242\n",
      "108 228 1.602372169494629 14.233136748012743 0.1991109848022461 251.808610200882 24.48656196128326\n",
      "109 229 8.671760559082031 40.58783164086821 9.216168642044067 611.0787858963013 63.70106545368021\n",
      "110 234 2.7093448638916016 11.532902813365316 2.311388731002808 122.00106906890868 19.42175135684609\n",
      "111 228 0.694469690322876 3.572155847884061 0.6797797679901123 206.2856819629669 14.629084187035135\n",
      "112 231 13.577345848083496 93.06860680600782 10.516506671905518 516.9946422576904 83.0502918291441\n",
      "113 227 8.312694311141968 26.39230655363478 5.107074737548828 524.8034620285034 46.68788918493268\n",
      "114 230 3.836666703224182 35.99942841011545 4.034478187561035 408.45878982543945 61.69710291021917\n",
      "115 231 7.7881224155426025 119.34592639729058 8.207119226455688 651.9706509113312 95.519249327087\n",
      "116 230 19.097517728805546 97.18221482090328 1.9136090278625488 828.075297832489 145.45956711684642\n",
      "117 228 6.68073034286499 18.99767582562932 6.66366982460022 228.7315096855164 29.39038703936946\n",
      "118 233 6.38218629360199 24.572477218930814 6.239512205123901 274.0355417728424 34.27176903855659\n",
      "119 227 1.216991662979126 9.181058823799772 1.2098548412322998 122.99516677856444 18.240520357759983\n",
      "120 231 14.802021384239197 89.8872387883983 13.702807664871216 521.9729063510895 81.57188675876688\n",
      "121 228 0.8887953758239746 5.9839815758822255 0.8949172496795654 75.44850659370422 8.783052044216602\n",
      "122 227 11.262148141860962 26.644483747986445 11.446885108947754 296.73921751976013 31.386309849074728\n",
      "123 229 9.194105863571167 30.47301016311979 5.504384279251099 577.7999031543732 50.444021079096544\n",
      "124 232 10.910569548606873 78.60094256544936 7.972716569900513 454.5634682178497 74.9087007666345\n",
      "125 230 0.8278020620346069 4.733747795353765 0.8961927890777588 36.26555633544922 6.097569060827614\n",
      "126 227 8.285063982009888 41.792776263232795 8.219484329223633 346.2703959941864 57.86225992830066\n",
      "127 233 1.8516784906387334 10.473897907355312 1.8508150577545168 123.62924551963806 18.985495411672222\n",
      "128 232 1.0306591987609863 3.774345776130413 1.0131607055664062 182.87041878700256 13.785042577843175\n",
      "129 229 0.3392587900161743 7.748697134084576 0.1772336959838867 189.04989910125727 17.2481908686536\n",
      "130 228 1.0008151531219482 6.7092010682089285 0.984455108642578 210.4319908618927 19.971839511706573\n",
      "131 229 5.6508060693740845 42.81280350164555 2.274811029434204 882.6924467086792 79.80449566195759\n",
      "132 225 24.098416566848755 125.02361134741041 10.705578327178957 812.015543460846 112.22905651725769\n",
      "133 232 6.863909006118774 38.17670321978372 6.934733629226685 238.1338231563568 48.21448223106008\n",
      "134 232 0.3114103078842163 4.366493097667036 0.2254960536956787 124.52342796325684 11.157137338745379\n",
      "135 231 0.3667069673538208 3.1559776103857793 0.3558194637298584 80.36720037460327 7.930749431250594\n",
      "136 228 3.4911205768585205 12.872139315856131 3.477272748947144 108.56292247772215 17.14029973110303\n",
      "137 226 2.3743420839309692 11.629464483894077 2.357557535171509 140.84221768379211 18.810667553151085\n",
      "138 233 6.539919853210449 21.457138395104796 3.803682327270508 421.6677234172821 42.28612278680878\n",
      "139 230 1.3059412240982056 5.964498753133028 1.397991418838501 77.27829551696777 7.947400228134564\n",
      "140 234 12.972070217132568 48.11342408310654 13.618361473083496 358.86529064178467 49.28563576137253\n",
      "141 231 19.70776605606079 45.52285309477802 19.71851229667664 227.8591673374176 35.64609699051045\n",
      "142 231 3.060199022293091 14.78552069086017 3.0348002910614014 249.79439306259155 30.071310035934985\n",
      "143 230 18.23871159553528 123.46095951225446 0.8330559730529785 873.8254110813141 140.41450609407184\n",
      "144 226 9.862430334091187 33.16805646377327 9.83920168876648 256.86318731307983 40.17918548102412\n",
      "145 230 0.1713464260101318 3.470118728927944 0.1578102111816406 131.79774141311646 11.649924159175766\n",
      "146 230 0.4665945768356323 2.5632105547448862 0.4615375995635986 22.80116581916809 3.594598259923185\n",
      "147 228 2.70374596118927 15.995738373513808 1.6238625049591064 248.50850248336792 29.017323828056917\n",
      "148 233 4.177105903625488 41.70398422883815 3.832716226577759 550.2805593013763 72.93686668982141\n",
      "149 231 16.796067476272583 77.39651892401956 13.144246101379396 378.63657689094543 72.82297612502774\n",
      "150 230 4.673532247543335 35.75100201109181 4.779495000839233 419.4638080596924 51.00539263612125\n",
      "151 233 0.27855515480041504 7.604756347099599 0.2649612426757812 125.61771774291992 15.52892959620375\n",
      "152 229 0.6669621467590332 2.3892141460851812 0.6513810157775879 51.19969534873962 4.600399215391463\n",
      "153 229 7.664286136627197 37.6813966093105 7.91940426826477 308.9274673461914 45.84625137353534\n",
      "154 229 2.7595338821411133 27.06811849519155 2.798395872116089 286.19928669929504 42.961182887388105\n",
      "155 232 6.104300260543823 37.75304815584216 6.4773547649383545 388.5006313323975 54.806607365855086\n",
      "156 232 2.6323719024658203 20.10403774319024 2.61113715171814 233.42871379852292 36.192503158250936\n",
      "157 226 0.18430626392364496 3.662107635388332 0.1907863616943359 82.80827474594116 10.382302713247324\n",
      "158 229 7.2269450426101685 69.3792762339896 2.250345230102539 289.97638511657715 65.65569543343378\n",
      "159 233 1.1127468347549438 46.495445415185756 0.7218091487884521 657.7414617538452 84.70835411264636\n",
      "160 229 5.657178997993469 23.302094099302998 5.731336832046509 130.39662432670593 25.807612956714884\n",
      "161 230 8.487364649772644 120.28708481892295 8.127492904663086 767.1353452205658 101.08179330889895\n",
      "162 225 6.001349568367004 48.500792519251505 6.424569845199585 563.6767053604126 77.57787495794861\n",
      "163 231 6.939703941345215 71.55494909885125 2.1854138374328613 634.807852268219 83.60216367097198\n",
      "164 231 3.7404625415802 22.36930914152236 3.826037168502808 249.51405715942383 33.959128811858825\n",
      "165 228 8.696645498275757 29.730545169428776 8.84803819656372 459.1758925914765 49.44670280743295\n",
      "166 230 25.596819162368774 74.48231284100076 11.845781564712524 358.54728960990906 70.94161192456585\n",
      "167 229 2.4007586240768433 35.765845014538826 2.382262706756592 359.2506437301636 48.542500037017696\n",
      "168 232 0.4179065227508545 2.6041598320007324 0.3863945007324219 62.30902719497681 6.828727982951353\n",
      "169 229 3.712475299835205 15.758839870644449 3.9931132793426514 153.64531755447388 23.703456909782606\n",
      "170 231 5.512607932090759 21.283116925846446 5.558483362197876 264.28207421302795 31.930914321641914\n",
      "171 233 1.9361424446105957 10.504061441053137 1.929032564163208 218.4919373989105 24.231984034298506\n",
      "172 234 7.443153738975525 81.95512522387709 2.385207414627075 598.2181715965271 85.19358207794829\n",
      "173 236 1.7837600708007812 9.270634558241246 0.7945759296417236 112.57887434959412 15.670041732918662\n",
      "174 230 2.9124265909194946 9.39484288174173 2.934699535369873 335.93525433540344 23.792474965967916\n",
      "175 230 1.1397892236709595 14.318943556495334 0.2618193626403808 135.85545468330383 22.646415017502026\n",
      "176 228 2.4101459980010986 13.860215922196707 1.3980929851531982 209.46514892578125 23.65271902118786\n",
      "177 231 3.8512006998062134 22.108906682951627 1.0215072631835938 270.04080605506897 38.274761280230436\n",
      "178 232 0.24165511131286616 6.463702484451491 0.2435355186462402 129.43451166152954 14.877716737131905\n",
      "179 231 13.338138341903687 35.36523454013841 13.647315502166748 262.44569396972656 33.449413464432695\n",
      "180 229 7.56574559211731 30.571914706167696 7.806947231292725 241.53387331962583 35.447661083031115\n",
      "181 226 0.40500462055206293 15.274104321952414 0.3258271217346191 465.8406684398651 39.11482197584801\n",
      "182 225 10.211458921432493 37.07300088670519 10.394190549850464 579.4852895736694 52.708073713107815\n",
      "183 234 7.018322587013245 75.85325447616414 2.7694809436798096 354.3851130008697 70.98458102845098\n",
      "184 230 3.5167590379714966 44.919779439594436 3.703669309616089 366.5129737854004 61.56197314842986\n",
      "185 231 0.4168636798858642 3.237778730722733 0.4083125591278076 165.33429384231567 11.51826715379958\n",
      "186 228 0.8907849788665771 6.504653860602462 0.8729982376098633 105.52725911140442 13.148336208647843\n",
      "187 228 33.46085178852081 164.7187758799185 14.315629005432127 704.1548817157745 135.1366689784838\n",
      "188 231 0.19507050514221191 12.268549926353224 0.193887710571289 214.29628944396973 25.67248119259615\n",
      "189 228 0.9979057312011721 9.607468829866042 0.893848180770874 67.05938982963562 12.326546249856277\n",
      "190 227 16.428144454956055 37.728459693261705 16.586246967315674 210.89579439163208 31.918344491938484\n",
      "191 230 10.03784465789795 81.91897363248079 6.2674195766448975 470.2880537509918 80.1268465090318\n",
      "192 234 17.095083832740784 93.18690280221466 10.58923888206482 727.5550312995911 98.06232170237726\n",
      "193 230 8.150933265686035 34.67311381049778 8.679991483688354 417.8757553100586 46.67583880993192\n",
      "194 228 12.0938138961792 79.11616543824212 8.724953889846802 457.2003836631775 77.01247975021201\n",
      "195 233 1.2089749574661255 6.067992845829976 1.2594635486602783 53.11949515342712 7.845615557438279\n",
      "196 231 0.5419982671737671 3.190135235394234 0.540708065032959 159.9200713634491 11.846790291234793\n",
      "197 230 4.692573070526123 19.852864567093228 4.691725969314575 197.8262825012207 29.21278495142181\n",
      "198 228 10.086978793144226 99.4422628210302 6.390735864639282 524.7883896827698 87.24583379032887\n",
      "199 227 5.917020559310913 31.56411805552008 5.606067657470703 259.770622253418 40.48466356775515\n",
      "200 232 5.254818797111511 62.36075970119443 1.0867276191711426 933.9080340862274 116.13112455975109\n",
      "201 232 2.9097938537597656 11.775970288391772 0.8598113059997559 168.2892289161682 24.57669751704992\n",
      "202 230 2.442924380302429 16.667455060585684 2.439401865005493 211.07663440704343 32.35537602212228\n",
      "203 224 19.312970399856567 104.8341786542109 1.11371111869812 891.161788225174 154.11478358225656\n",
      "204 233 6.308782577514648 27.9266597528826 3.266587972640991 321.1365110874176 44.84496899475659\n",
      "205 233 10.210371851921082 191.3615918752973 14.341096878051758 906.7794389724731 125.77844484327738\n",
      "206 232 11.904564499855042 167.75779772318643 11.35116982460022 586.7921531200409 107.18132489371709\n",
      "207 229 8.020141959190369 73.65458152804312 2.994699716567993 382.7864911556244 71.24679552285926\n",
      "208 228 28.87237274646759 129.51918113440797 2.4284181594848637 814.044926404953 151.0572571930171\n",
      "209 233 1.209226369857788 9.884919844983473 0.85870361328125 145.61238813400269 20.293931831866278\n",
      "210 230 2.6904942989349365 22.260309454669123 2.78587007522583 289.0114107131958 37.83752220291924\n",
      "211 225 27.091694831848145 197.8480227226681 2.295079708099365 999.4311907291412 184.47560437300487\n",
      "212 228 4.390819191932678 33.94401585323769 4.138484716415405 275.92891931533813 49.70506309650289\n",
      "213 230 12.556489944458008 171.29459350834722 14.682502746582031 583.2792727947235 108.71331556941097\n",
      "214 232 12.04014778137207 173.60580073142873 12.217249870300291 823.8663127422333 135.31651812420068\n",
      "215 230 8.096134185791016 71.24398886639139 3.3256964683532715 308.01374077796936 74.18000454535624\n",
      "216 229 7.75724184513092 80.83514411688893 1.7163128852844238 790.6862890720367 115.18499273709824\n",
      "217 232 0.949327826499939 9.426044526798972 0.8610653877258301 204.5935389995575 20.79076111902355\n",
      "218 232 2.151177763938904 30.20744282623817 1.9332664012908936 832.4951300621033 64.56199898646686\n",
      "219 229 1.8581289052963257 169.4209426157339 1.938891649246216 947.9395830631256 191.98773885088886\n",
      "220 227 2.1293857097625732 43.5608831775346 1.6380760669708252 794.4643445014954 81.44395205261122\n",
      "221 227 23.26487445831299 114.624406563553 5.687616348266602 737.9855194091797 117.23947250570339\n",
      "222 229 30.566919445991516 180.73973431857914 2.038228750228882 987.1521043777466 177.26050295941087\n",
      "223 236 13.651198506355286 48.28691274230763 5.966356039047241 375.4493668079376 52.38134494391125\n",
      "224 226 6.648057699203491 78.56346160542648 1.6024835109710691 914.7680704593658 114.83711518833982\n",
      "225 232 2.4115813970565796 11.273237472978131 0.8617382049560547 189.9431681632996 24.564697231030156\n",
      "226 227 4.757903575897217 14.87595114833983 2.4426379203796387 190.04514503479004 27.034597873358944\n",
      "227 222 14.828880071640015 164.82067516365566 2.045182228088379 997.106837272644 182.52986905327575\n",
      "228 231 7.921910643577576 34.733868298592505 3.0170702934265137 600.9451041221619 59.47092767322597\n",
      "229 228 10.580158472061157 168.6803647833958 11.361130475997925 794.4570212364197 119.64698229399352\n",
      "230 230 9.38940966129303 174.99319846733758 12.703486919403076 700.8729162216187 115.05487986794496\n",
      "231 233 7.222126603126526 69.22549312411459 3.2608842849731445 439.47985768318176 76.05706391059603\n",
      "232 229 39.5774781703949 149.33348794170863 3.005999088287353 943.9175624847412 172.21901110041333\n",
      "233 231 3.0603712797164917 11.024923584677957 0.8706722259521484 127.09760308265686 20.442996618267124\n",
      "234 226 6.144931077957153 20.441954690798195 2.4381346702575684 252.4746162891388 34.73765333854583\n",
      "235 215 46.337228298187256 300.72596452291623 5.124367952346802 965.9088985919952 230.79761520043542\n",
      "236 234 3.8934144973754883 27.977809882571556 3.006927967071533 205.1373188495636 42.879503604409145\n",
      "237 229 12.338775038719177 184.01472043574637 12.171016454696655 735.2916848659515 128.97356512224243\n",
      "238 228 12.147811770439148 182.7452677038678 12.287813663482666 839.0701305866241 127.09493107673413\n",
      "239 230 8.366368889808655 79.14870930028998 2.9624814987182617 500.67477464675903 80.64110452120165\n"
     ]
    }
   ],
   "source": [
    "for i, rows in concurrency_df.groupby(\"query_idx\"):\n",
    "    runtime = rows[\"runtime\"].values\n",
    "    print(i, len(rows), isolated_rt_cache[i], np.mean(runtime), np.min(runtime), np.max(runtime), np.std(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baed6340-0010-4701-ab3a-fe7dbd0f05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "train_idx = np.random.choice(len(concurrency_df), size=int(0.8 * len(concurrency_df)), replace=False)\n",
    "test_idx = [i for i in range(len(concurrency_df)) if i not in train_idx]\n",
    "eval_trace_df = concurrency_df.iloc[test_idx]\n",
    "eval_trace_df = eval_trace_df[eval_trace_df['num_concurrent_queries'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83529b-a7d2-4f17-a7cf-25749e882269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import l1_loss\n",
    "import scipy.optimize as optimization\n",
    "from torch.utils.data import DataLoader\n",
    "from models.concurrency.base_model import ConcurPredictor\n",
    "from models.concurrency.utils import QueryFeatureDataset, SimpleNet\n",
    "from models.concurrency.analytical_functions import simple_queueing_func, interaction_func_torch, interaction_func_scipy\n",
    "from parser.utils import load_json, dfs_cardinality, estimate_scan_in_mb\n",
    "\n",
    "\n",
    "class SimpleFitCurve(ConcurPredictor):\n",
    "    \"\"\"\n",
    "    Simple fit curve model for runtime prediction with concurrency\n",
    "    runtime = queue_time(num_concurrency) + alpha(num_concurrency) * isolated_runtime\n",
    "            = (a1 * max(num_concurrency-b1, 0)) + (1 + a2*min(num_concurrency, b1)) * isolated_runtime\n",
    "    optimize a1, b1, b2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.isolated_rt_cache = dict()\n",
    "        self.use_train = True\n",
    "        self.a1_global = 0\n",
    "        self.a1 = dict()\n",
    "        self.b1_global = 0\n",
    "        self.b1 = dict()\n",
    "        self.a2_global = 0\n",
    "        self.a2 = dict()\n",
    "\n",
    "    def train(self, trace_df, use_train=True, isolated_trace_df=None):\n",
    "        self.use_train = use_train\n",
    "        self.get_isolated_runtime_cache(trace_df, isolated_trace_df)\n",
    "        concurrent_df = trace_df[trace_df[\"num_concurrent_queries\"] > 0]\n",
    "\n",
    "        global_y = []\n",
    "        global_x = []\n",
    "        global_ir = []\n",
    "        for i, rows in concurrent_df.groupby(\"query_idx\"):\n",
    "            if i not in self.isolated_rt_cache:\n",
    "                continue\n",
    "            isolated_rt = self.isolated_rt_cache[i]\n",
    "            concurrent_rt = rows[\"runtime\"].values\n",
    "            if use_train:\n",
    "                num_concurrency = rows[\"num_concurrent_queries_train\"].values\n",
    "            else:\n",
    "                num_concurrency = rows[\"num_concurrent_queries\"].values\n",
    "            if len(num_concurrency) < 10:\n",
    "                continue\n",
    "            global_y.append(concurrent_rt)\n",
    "            global_x.append(num_concurrency)\n",
    "            global_ir.append(np.ones(len(num_concurrency)) * isolated_rt)\n",
    "            fit, _ = optimization.curve_fit(\n",
    "                simple_queueing_func,\n",
    "                (num_concurrency, np.ones(len(num_concurrency)) * isolated_rt),\n",
    "                concurrent_rt,\n",
    "                np.array([5, 0.1, 20]),\n",
    "            )\n",
    "            self.a1[i] = fit[0]\n",
    "            self.a2[i] = fit[1]\n",
    "            self.b1[i] = fit[2]\n",
    "        global_y = np.concatenate(global_y)\n",
    "        global_x = np.concatenate(global_x)\n",
    "        global_ir = np.concatenate(global_ir)\n",
    "        fit, _ = optimization.curve_fit(\n",
    "            simple_queueing_func,\n",
    "            (global_x, global_ir),\n",
    "            global_y,\n",
    "            np.array([5, 0.1, 20]),\n",
    "        )\n",
    "        self.a1_global = fit[0]\n",
    "        self.a2_global = fit[1]\n",
    "        self.b1_global = fit[2]\n",
    "\n",
    "    def predict(self, eval_trace_df, use_global=False):\n",
    "        predictions = dict()\n",
    "        labels = dict()\n",
    "        for i, rows in eval_trace_df.groupby(\"query_idx\"):\n",
    "            if i not in self.isolated_rt_cache or i not in self.a1:\n",
    "                continue\n",
    "            isolated_rt = self.isolated_rt_cache[i]\n",
    "            label = rows[\"runtime\"].values\n",
    "            labels[i] = label\n",
    "            if self.use_train:\n",
    "                num_concurrency = rows[\"num_concurrent_queries_train\"].values\n",
    "            else:\n",
    "                num_concurrency = rows[\"num_concurrent_queries\"].values\n",
    "            x = (num_concurrency, np.ones(len(num_concurrency)) * isolated_rt)\n",
    "            if use_global:\n",
    "                pred = simple_queueing_func(\n",
    "                    x, self.a1_global, self.a2_global, self.b1_global\n",
    "                )\n",
    "            else:\n",
    "                pred = simple_queueing_func(x, self.a1[i], self.a2[i], self.b1[i])\n",
    "            pred = np.maximum(pred, 0.001)\n",
    "            predictions[i] = pred\n",
    "        return predictions, labels\n",
    "\n",
    "\n",
    "def fit_curve_loss_torch(x, y, params, constrain, loss_func=\"soft_l1\", penalties=None):\n",
    "    pred = interaction_func_torch(x, *params)\n",
    "    lb = constrain.lb\n",
    "    ub = constrain.ub\n",
    "    if loss_func == \"mae\":\n",
    "        loss = torch.abs(pred - y)\n",
    "    elif loss_func == \"mse\":\n",
    "        loss = (pred - y) ** 2\n",
    "    elif loss_func == \"soft_l1\":\n",
    "        loss = torch.sqrt(1 + (pred - y) ** 2) - 1\n",
    "    else:\n",
    "        assert False, f\"loss func {loss_func} not implemented\"\n",
    "    loss = torch.mean(loss)\n",
    "    for i, p in enumerate(params):\n",
    "        if penalties is not None:\n",
    "            penalty = penalties[i]\n",
    "        else:\n",
    "            penalty = 1\n",
    "        pen = torch.exp(penalty * (p - ub[i])) + torch.exp(-1 * penalty * (p - lb[i]))\n",
    "        loss += pen\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ComplexFitCurve(ConcurPredictor):\n",
    "    \"\"\"\n",
    "    Complex fit curve model for runtime prediction with concurrency\n",
    "    See interaction_func_scipy for detailed analytical functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_column_store=False, opt_method='scipy'):\n",
    "        \"\"\"\n",
    "\n",
    "        :param is_column_store:\n",
    "        :param opt_method:\n",
    "        \"\"\"\n",
    "        # indicate whether the DBMS is a column_store\n",
    "        #\n",
    "        super().__init__()\n",
    "        self.isolated_rt_cache = dict()\n",
    "        self.average_rt_cache = dict()\n",
    "        self.query_info = dict()\n",
    "        self.db_stats = None\n",
    "        self.table_sizes = dict()\n",
    "        self.table_sizes_by_index = dict()\n",
    "        self.table_nrows_by_index = dict()\n",
    "        self.table_column_map = dict()\n",
    "        self.use_pre_info = False\n",
    "        self.use_post_info = False\n",
    "        self.is_column_store = is_column_store\n",
    "        self.opt_method = opt_method\n",
    "        self.batch_size = 1024\n",
    "        self.analytic_params = [0.5, 20, 2, 0.2, 0.5, 0.5, 0.1, 0.2, 0.8, 0.2, 10, 200, 16000]\n",
    "        self.bound = optimization.Bounds(\n",
    "            [0.1, 10, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.5, 0.05, 2, 20, 10000],\n",
    "            [1, 200, 2, 1, 0.9, 0.9, 0.5, 0.5, 0.95, 0.4, 20, 2000, 50000],\n",
    "        )\n",
    "        self.constrain = optimization.Bounds(\n",
    "            [0.1, 10, 0.1, 0.01, 0.01, 0.01, 0.01, 0.1, 0.5, 0.05, 2, 20, 10000],\n",
    "            [1, 200, 2, 1, 0.9, 0.9, 0.5, 0.5, 0.95, 0.4, 20, 2000, 50000],\n",
    "        )\n",
    "        self.penalty = [100, 0.1, 100, 100, 100, 100, 100, 100, 100, 100, 1, 0.1, 0.01]\n",
    "        self.loss_func = \"soft_l1\"\n",
    "        self.model = None\n",
    "        self.analytic_func = None\n",
    "\n",
    "    def _compute_table_size(self):\n",
    "        for col in self.db_stats[\"column_stats\"]:\n",
    "            table = col[\"tablename\"]\n",
    "            if table not in self.table_column_map:\n",
    "                self.table_sizes[table] = 0\n",
    "                self.table_column_map[table] = []\n",
    "            self.table_column_map[table].append(col[\"attname\"])\n",
    "            if col[\"avg_width\"] is not None and col[\"avg_width\"] > 0:\n",
    "                self.table_sizes[table] += col[\"avg_width\"]\n",
    "        all_table_names = [t[\"relname\"] for t in self.db_stats[\"table_stats\"]]\n",
    "        for table in self.table_sizes:\n",
    "            if table in all_table_names:\n",
    "                idx = all_table_names.index(table)\n",
    "                num_tuples = self.db_stats[\"table_stats\"][idx][\"reltuples\"]\n",
    "                self.table_nrows_by_index[idx] = num_tuples\n",
    "                size_in_mb = (num_tuples * self.table_sizes[table]) / (1024 * 1024)\n",
    "                self.table_sizes[table] = size_in_mb\n",
    "                self.table_sizes_by_index[idx] = size_in_mb\n",
    "\n",
    "    def pre_process_queries(\n",
    "        self, parsed_queries_path, with_width=True, use_true_card=False\n",
    "    ):\n",
    "        plans = load_json(parsed_queries_path, namespace=False)\n",
    "        self.db_stats = plans[\"database_stats\"]\n",
    "        self._compute_table_size()\n",
    "        self.query_info = dict()\n",
    "        for i in range(len(plans[\"sql_queries\"])):\n",
    "            curr_query_info = dict()\n",
    "            curr_query_info[\"sql\"] = plans[\"sql_queries\"][i]\n",
    "            all_cardinality = []\n",
    "            dfs_cardinality(\n",
    "                plans[\"parsed_plans\"][i], all_cardinality, with_width, use_true_card\n",
    "            )\n",
    "            curr_query_info[\"all_cardinality\"] = all_cardinality\n",
    "            est_scan, est_scan_per_table = estimate_scan_in_mb(\n",
    "                self.db_stats,\n",
    "                plans[\"parsed_queries\"][i],\n",
    "                use_true_card,\n",
    "                self.is_column_store,\n",
    "            )\n",
    "            curr_query_info[\"est_scan\"] = est_scan\n",
    "            curr_query_info[\"est_scan_per_table\"] = est_scan_per_table\n",
    "            self.query_info[i] = curr_query_info\n",
    "\n",
    "    def estimate_data_share_percentage(self, idx, concur_info, pre_exec_info=None):\n",
    "        # TODO: make it smarter by considering buffer pool behavior\n",
    "        curr_scan = self.query_info[idx][\"est_scan_per_table\"]\n",
    "        curr_total_scan = self.query_info[idx][\"est_scan\"]\n",
    "        if pre_exec_info is not None:\n",
    "            concur_info = concur_info + pre_exec_info\n",
    "        all_shared_scan = 0\n",
    "        for table in curr_scan:\n",
    "            table_size = self.table_sizes_by_index[table]\n",
    "            table_shared_scan = 0\n",
    "            for c in concur_info:\n",
    "                concur_scan = self.query_info[c[0]][\"est_scan_per_table\"]\n",
    "                if table in concur_scan:\n",
    "                    concur_scan_perc = concur_scan[table] / table_size\n",
    "                    overlap_scan = concur_scan_perc * curr_scan[table]\n",
    "                    table_shared_scan += overlap_scan\n",
    "            table_shared_scan = min(table_shared_scan, curr_scan[table])\n",
    "            all_shared_scan += table_shared_scan\n",
    "        return min(all_shared_scan / curr_total_scan, 1.0)\n",
    "\n",
    "    def featurize_data(self, concurrent_df):\n",
    "        global_y = []\n",
    "        global_isolated_runtime = []\n",
    "        global_avg_runtime = []\n",
    "        global_num_concurrency = []\n",
    "        global_sum_concurrent_runtime = []\n",
    "        global_est_scan = []\n",
    "        global_est_concurrent_scan = []\n",
    "        global_scan_sharing_percentage = []\n",
    "        global_max_est_card = []\n",
    "        global_avg_est_card = []\n",
    "        global_max_concurrent_card = []\n",
    "        global_avg_concurrent_card = []\n",
    "        global_query_idx = dict()\n",
    "        start = 0\n",
    "        for i, rows in concurrent_df.groupby(\"query_idx\"):\n",
    "            if (\n",
    "                i not in self.isolated_rt_cache\n",
    "                or i not in self.query_info\n",
    "                or i not in self.average_rt_cache\n",
    "            ):\n",
    "                continue\n",
    "            concurrent_rt = rows[\"runtime\"].values\n",
    "            query_info = self.query_info[i]\n",
    "            n_rows = len(rows)\n",
    "            if self.use_pre_info:\n",
    "                num_concurrency = rows[\"num_concurrent_queries_train\"].values\n",
    "                concur_info = rows[\"concur_info_train\"].values\n",
    "            else:\n",
    "                num_concurrency = rows[\"num_concurrent_queries\"].values\n",
    "                concur_info = rows[\"concur_info\"].values\n",
    "            pre_exec_info = rows[\"pre_exec_info\"].values\n",
    "\n",
    "            global_query_idx[i] = (start, start + n_rows)\n",
    "            start += n_rows\n",
    "            global_y.append(concurrent_rt)\n",
    "            global_isolated_runtime.append(np.ones(n_rows) * self.isolated_rt_cache[i])\n",
    "            global_avg_runtime.append(np.ones(n_rows) * self.average_rt_cache[i])\n",
    "            global_num_concurrency.append(num_concurrency)\n",
    "            global_est_scan.append(np.ones(n_rows) * query_info[\"est_scan\"])\n",
    "            global_max_est_card.append(\n",
    "                np.ones(n_rows) * np.max(query_info[\"all_cardinality\"]) / (1024 * 1024)\n",
    "            )\n",
    "            global_avg_est_card.append(\n",
    "                np.ones(n_rows)\n",
    "                * np.average(query_info[\"all_cardinality\"])\n",
    "                / (1024 * 1024)\n",
    "            )\n",
    "            for j in range(n_rows):\n",
    "                sum_concurrent_runtime = 0\n",
    "                sum_concurrent_scan = 0\n",
    "                concurrent_card = []\n",
    "                for c in concur_info[j]:\n",
    "                    if c[0] in self.average_rt_cache:\n",
    "                        sum_concurrent_runtime += self.average_rt_cache[c[0]]\n",
    "                    else:\n",
    "                        print(c[0])\n",
    "                    if c[0] in self.query_info:\n",
    "                        sum_concurrent_scan += self.query_info[c[0]][\"est_scan\"]\n",
    "                        concurrent_card.extend(self.query_info[c[0]][\"all_cardinality\"])\n",
    "                    else:\n",
    "                        print(c[0])\n",
    "\n",
    "                global_sum_concurrent_runtime.append(sum_concurrent_runtime)\n",
    "                global_est_concurrent_scan.append(sum_concurrent_scan)\n",
    "                if len(concurrent_card) == 0:\n",
    "                    global_max_concurrent_card.append(0)\n",
    "                    global_avg_concurrent_card.append(0)\n",
    "                else:\n",
    "                    global_max_concurrent_card.append(\n",
    "                        np.max(concurrent_card) / (1024 * 1024)\n",
    "                    )\n",
    "                    global_avg_concurrent_card.append(\n",
    "                        np.average(concurrent_card) / (1024 * 1024)\n",
    "                    )\n",
    "                global_scan_sharing_percentage.append(\n",
    "                    self.estimate_data_share_percentage(\n",
    "                        i, concur_info[j], pre_exec_info[j]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        global_y = np.concatenate(global_y)\n",
    "        global_isolated_runtime = np.concatenate(global_isolated_runtime)\n",
    "        global_avg_runtime = np.concatenate(global_avg_runtime)\n",
    "        global_num_concurrency = np.concatenate(global_num_concurrency)\n",
    "        global_est_scan = np.concatenate(global_est_scan)\n",
    "        global_max_est_card = np.concatenate(global_max_est_card)\n",
    "        global_avg_est_card = np.concatenate(global_avg_est_card)\n",
    "        global_sum_concurrent_runtime = np.asarray(global_sum_concurrent_runtime)\n",
    "        global_est_concurrent_scan = np.asarray(global_est_concurrent_scan)\n",
    "        global_max_concurrent_card = np.asarray(global_max_concurrent_card)\n",
    "        global_avg_concurrent_card = np.asarray(global_avg_concurrent_card)\n",
    "        global_scan_sharing_percentage = np.asarray(global_scan_sharing_percentage)\n",
    "        feature = (\n",
    "            global_isolated_runtime,\n",
    "            global_avg_runtime,\n",
    "            global_num_concurrency,\n",
    "            global_sum_concurrent_runtime,\n",
    "            global_est_scan,\n",
    "            global_est_concurrent_scan,\n",
    "            global_scan_sharing_percentage,\n",
    "            global_max_est_card,\n",
    "            global_avg_est_card,\n",
    "            global_max_concurrent_card,\n",
    "            global_avg_concurrent_card,\n",
    "        )\n",
    "        if self.opt_method == \"torch\" or self.opt_method == \"nn\":\n",
    "            feature = list(feature)\n",
    "            for i in range(len(feature)):\n",
    "                feature[i] = torch.from_numpy(feature[i])\n",
    "            feature = tuple(feature)\n",
    "            global_y = torch.from_numpy(global_y)\n",
    "        return feature, global_y, global_query_idx\n",
    "\n",
    "    def train(self, trace_df, use_train=True, isolated_trace_df=None, analytic_func=None):\n",
    "        if analytic_func is None:\n",
    "            analytic_func = interaction_func_scipy\n",
    "        self.analytic_func = analytic_func\n",
    "        self.use_pre_info = use_train\n",
    "        self.get_isolated_runtime_cache(\n",
    "            trace_df, isolated_trace_df, get_avg_runtime=True\n",
    "        )\n",
    "        concurrent_df = trace_df[trace_df[\"num_concurrent_queries\"] > 0]\n",
    "        feature, label, _ = self.featurize_data(concurrent_df)\n",
    "\n",
    "        initial_param_value = np.asarray(self.analytic_params)\n",
    "        if self.opt_method == \"scipy\":\n",
    "            fit, _ = optimization.curve_fit(\n",
    "                self.analytic_func,\n",
    "                feature,\n",
    "                label,\n",
    "                initial_param_value,\n",
    "                bounds=self.bound,\n",
    "                jac=\"3-point\",\n",
    "                method=\"trf\",\n",
    "                loss=\"soft_l1\",\n",
    "                verbose=1\n",
    "            )\n",
    "            self.analytic_params = list(fit)\n",
    "        elif self.opt_method == \"torch\":\n",
    "            torch_analytic_params = []\n",
    "            torch_analytic_params_lr = []\n",
    "            for p in self.analytic_params:\n",
    "                if p == 10:\n",
    "                    t_p = torch.tensor(float(p), requires_grad=False)\n",
    "                else:\n",
    "                    t_p = torch.tensor(float(p), requires_grad=True)\n",
    "                torch_analytic_params.append(t_p)\n",
    "                torch_analytic_params_lr.append({'params': t_p, 'lr': 0.01 * p ** 0.3})\n",
    "            optimizer = optim.Adam(torch_analytic_params_lr, weight_decay=2e-5)\n",
    "            dataset = QueryFeatureDataset(feature, label)\n",
    "            train_dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            for epoch in range(200):\n",
    "                for X, y in train_dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = fit_curve_loss_torch(X, y, torch_analytic_params,\n",
    "                                                self.constrain, loss_func=self.loss_func, penalties=self.penalty)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                if epoch % 10 == 0:\n",
    "                    print(epoch, loss.item())\n",
    "                    print(torch_analytic_params)\n",
    "            for i in range(len(self.analytic_params)):\n",
    "                self.analytic_params[i] = torch_analytic_params[i].detach()\n",
    "        elif self.opt_method == \"nn\":\n",
    "            dataset = QueryFeatureDataset(feature, label)\n",
    "            train_dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            self.model = SimpleNet(len(feature))\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=0.01, weight_decay=2e-5)\n",
    "            for epoch in range(200):\n",
    "                for X, y in train_dataloader:\n",
    "                    X = torch.stack(X).float()\n",
    "                    X = torch.transpose(X, 0, 1)\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = self.model(X)\n",
    "                    pred = pred.reshape(-1)\n",
    "                    loss = l1_loss(pred, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                if epoch % 10 == 0:\n",
    "                    print(epoch, loss.item())\n",
    "        elif self.opt_method == \"xgboost\":\n",
    "            feature = np.stack(feature).T\n",
    "            model = XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                max_depth=8,\n",
    "                eta=0.2,\n",
    "                subsample=1.0,\n",
    "                eval_metric=\"mae\",\n",
    "                early_stopping_rounds=100,\n",
    "            )\n",
    "            train_idx = np.random.choice(\n",
    "                len(feature), size=int(0.8 * len(feature)), replace=False\n",
    "            )\n",
    "            val_idx = [i for i in range(len(feature)) if i not in train_idx]\n",
    "            model.fit(\n",
    "                feature[train_idx],\n",
    "                label[train_idx],\n",
    "                eval_set=[(feature[val_idx], label[val_idx])],\n",
    "                verbose=False,\n",
    "            )\n",
    "            self.model = model\n",
    "        else:\n",
    "            assert False, f\"unrecognized optimization method {self.opt_method}\"\n",
    "\n",
    "    def predict(self, eval_trace_df, use_global=False, return_per_query=True):\n",
    "        if self.analytic_func is None:\n",
    "            self.analytic_func = interaction_func_scipy\n",
    "        feature, labels, query_idx = self.featurize_data(eval_trace_df)\n",
    "        if self.opt_method == \"scipy\":\n",
    "            preds = self.analytic_func(\n",
    "                feature,\n",
    "                *self.analytic_params\n",
    "            )\n",
    "        elif self.opt_method == \"torch\":\n",
    "            preds = interaction_func_torch(\n",
    "                feature,\n",
    "                *self.analytic_params\n",
    "            )\n",
    "            preds = preds.numpy()\n",
    "            labels = labels.numpy()\n",
    "        elif self.opt_method == \"nn\":\n",
    "            feature = torch.stack(feature).float()\n",
    "            feature = torch.transpose(feature, 0, 1)\n",
    "            preds = self.model(feature)\n",
    "            preds = preds.reshape(-1)\n",
    "            preds = preds.detach().numpy()\n",
    "            labels = labels.numpy()\n",
    "        elif self.opt_method == \"xgboost\":\n",
    "            feature = np.stack(feature).T\n",
    "            preds = self.model.predict(feature)\n",
    "            preds = np.maximum(preds, 0.001)\n",
    "        else:\n",
    "            assert False, f\"unrecognized optimization method {self.opt_method}\"\n",
    "        if return_per_query:\n",
    "            preds_per_query = dict()\n",
    "            labels_per_query = dict()\n",
    "            for i in query_idx:\n",
    "                start, end = query_idx[i]\n",
    "                preds_per_query[i] = preds[start:end]\n",
    "                labels_per_query[i] = labels[start:end]\n",
    "            return preds_per_query, labels_per_query\n",
    "        else:\n",
    "            return preds, labels\n",
    "\n",
    "\n",
    "class ComplexFitCurveSeparation(ComplexFitCurve):\n",
    "    \"\"\"\n",
    "    Complex fit curve model for runtime prediction with concurrency\n",
    "    See interaction_func_scipy for detailed analytical functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_column_store=False, opt_method='scipy'):\n",
    "        super().__init__(is_column_store, opt_method)\n",
    "        self.analytic_params = [0.3, 0.5, 20, 2, 0.2, 0.1, 0.3, 0.3, 0.3, 0.3, 0.1, 0.2, 0.8, 0.2, 10, 200, 16000]\n",
    "        self.bound = optimization.Bounds(\n",
    "            [0.1, 0.1, 10, 0.01, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.5, 0.05, 2, 20, 10000],\n",
    "            [1, 1, 200, 2, 1, 0.9, 0.9, 0.9, 0.5, 0.5, 0.5, 0.5, 0.95, 0.4, 20, 2000, 50000],\n",
    "        )\n",
    "        self.constrain = optimization.Bounds(\n",
    "            [0.1, 0.1, 10, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.5, 0.05, 2, 20, 10000],\n",
    "            [1, 1, 200, 2, 1, 0.9, 0.9, 0.9, 0.5, 0.5, 0.5, 0.5, 0.95, 0.4, 20, 2000, 50000],\n",
    "        )\n",
    "        self.penalty = [100, 100, 0.1, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1, 0.1, 0.01]\n",
    "\n",
    "    def featurize_data(self, concurrent_df):\n",
    "        global_y = []\n",
    "        global_isolated_runtime = []\n",
    "        global_avg_runtime = []\n",
    "        global_num_concurrency_pre = []\n",
    "        global_num_concurrency_post = []\n",
    "        global_sum_concurrent_runtime_pre = []\n",
    "        global_sum_concurrent_runtime_post = []\n",
    "        global_avg_time_elapsed_pre = []\n",
    "        global_sum_time_overlap_post = []\n",
    "        global_est_scan = []\n",
    "        global_est_concurrent_scan_pre = []\n",
    "        global_est_concurrent_scan_post = []\n",
    "        global_scan_sharing_percentage = []\n",
    "        global_max_est_card = []\n",
    "        global_avg_est_card = []\n",
    "        global_max_concurrent_card_pre = []\n",
    "        global_max_concurrent_card_post = []\n",
    "        global_avg_concurrent_card_pre = []\n",
    "        global_avg_concurrent_card_post = []\n",
    "        global_query_idx = dict()\n",
    "        start = 0\n",
    "        for i, rows in concurrent_df.groupby(\"query_idx\"):\n",
    "            if (\n",
    "                    i not in self.isolated_rt_cache\n",
    "                    or i not in self.query_info\n",
    "                    or i not in self.average_rt_cache\n",
    "            ):\n",
    "                continue\n",
    "            concurrent_rt = rows[\"runtime\"].values\n",
    "            start_time = rows[\"start_time\"].values\n",
    "            end_time = rows[\"end_time\"].values\n",
    "            query_info = self.query_info[i]\n",
    "            n_rows = len(rows)\n",
    "            num_concurrency_pre = rows[\"num_concurrent_queries_train\"].values\n",
    "            global_num_concurrency_pre.append(num_concurrency_pre)\n",
    "            concur_info_prev = rows[\"concur_info_train\"].values\n",
    "            full_concur_info = rows[\"concur_info\"].values\n",
    "            concur_info_post = []\n",
    "            for j in range(len(full_concur_info)):\n",
    "                new_info = [c for c in full_concur_info[j] if c not in full_concur_info[j]]\n",
    "                concur_info_post.append(new_info)\n",
    "            pre_exec_info = rows[\"pre_exec_info\"].values\n",
    "\n",
    "            global_query_idx[i] = (start, start + n_rows)\n",
    "            start += n_rows\n",
    "            global_y.append(concurrent_rt)\n",
    "            global_isolated_runtime.append(np.ones(n_rows) * self.isolated_rt_cache[i])\n",
    "            global_avg_runtime.append(np.ones(n_rows) * self.average_rt_cache[i])\n",
    "            global_est_scan.append(np.ones(n_rows) * query_info[\"est_scan\"])\n",
    "            global_max_est_card.append(\n",
    "                np.ones(n_rows) * np.max(query_info[\"all_cardinality\"]) / (1024 * 1024)\n",
    "            )\n",
    "            global_avg_est_card.append(\n",
    "                np.ones(n_rows)\n",
    "                * np.average(query_info[\"all_cardinality\"])\n",
    "                / (1024 * 1024)\n",
    "            )\n",
    "            for j in range(n_rows):\n",
    "                sum_concurrent_runtime_pre = 0\n",
    "                sum_concurrent_runtime_post = 0\n",
    "                sum_concurrent_scan_pre = 0\n",
    "                sum_concurrent_scan_post = 0\n",
    "                avg_time_elapsed_pre = 0\n",
    "                sum_time_overlap_post = 0\n",
    "                concurrent_card_pre = []\n",
    "                concurrent_card_post = []\n",
    "                for c in full_concur_info[j]:\n",
    "                    if c[0] in self.average_rt_cache:\n",
    "                        if c in concur_info_prev[j]:\n",
    "                            sum_concurrent_runtime_pre += self.average_rt_cache[c[0]]\n",
    "                            avg_time_elapsed_pre += (start_time[j] - c[1])\n",
    "                        else:\n",
    "                            sum_concurrent_runtime_post += self.average_rt_cache[c[0]]\n",
    "                            # TODO: this is not practical, make it an estimation\n",
    "                            sum_time_overlap_post += (end_time[j] - c[1])\n",
    "                    else:\n",
    "                        print(c[0])\n",
    "                    if c[0] in self.query_info:\n",
    "                        if c in concur_info_prev[j]:\n",
    "                            sum_concurrent_scan_pre += self.query_info[c[0]][\"est_scan\"]\n",
    "                            concurrent_card_pre.extend(self.query_info[c[0]][\"all_cardinality\"])\n",
    "                        else:\n",
    "                            sum_concurrent_scan_post += self.query_info[c[0]][\"est_scan\"]\n",
    "                            concurrent_card_post.extend(self.query_info[c[0]][\"all_cardinality\"])\n",
    "                    else:\n",
    "                        print(c[0])\n",
    "\n",
    "                global_sum_concurrent_runtime_pre.append(sum_concurrent_runtime_pre)\n",
    "                global_avg_time_elapsed_pre.append(avg_time_elapsed_pre / len(concur_info_prev[j]))\n",
    "                global_est_concurrent_scan_pre.append(sum_concurrent_scan_pre)\n",
    "                if len(concurrent_card_pre) == 0:\n",
    "                    global_max_concurrent_card_pre.append(0)\n",
    "                    global_avg_concurrent_card_pre.append(0)\n",
    "                else:\n",
    "                    global_max_concurrent_card_pre.append(\n",
    "                        np.max(concurrent_card_pre) / (1024 * 1024)\n",
    "                    )\n",
    "                    global_avg_concurrent_card_pre.append(\n",
    "                        np.average(concurrent_card_pre) / (1024 * 1024)\n",
    "                    )\n",
    "                # TODO: may be able to change concur_info_prev to full_concur_info?\n",
    "                global_scan_sharing_percentage.append(\n",
    "                    self.estimate_data_share_percentage(\n",
    "                        i, concur_info_prev[j], pre_exec_info[j]\n",
    "                    )\n",
    "                )\n",
    "                if self.use_pre_info:\n",
    "                    global_sum_concurrent_runtime_post.append(0)\n",
    "                    global_est_concurrent_scan_post.append(0)\n",
    "                    global_sum_time_overlap_post.append(0)\n",
    "                    global_max_concurrent_card_post.append(0)\n",
    "                    global_avg_concurrent_card_post.append(0)\n",
    "                else:\n",
    "                    global_sum_concurrent_runtime_post.append(sum_concurrent_runtime_post)\n",
    "                    global_est_concurrent_scan_post.append(sum_concurrent_scan_post)\n",
    "                    global_sum_time_overlap_post.append(sum_time_overlap_post)\n",
    "                    if len(concurrent_card_post) == 0:\n",
    "                        global_max_concurrent_card_post.append(0)\n",
    "                        global_avg_concurrent_card_post.append(0)\n",
    "                    else:\n",
    "                        global_max_concurrent_card_post.append(\n",
    "                            np.max(concurrent_card_post) / (1024 * 1024)\n",
    "                        )\n",
    "                        global_avg_concurrent_card_post.append(\n",
    "                            np.average(concurrent_card_post) / (1024 * 1024)\n",
    "                        )\n",
    "\n",
    "            if self.use_pre_info:\n",
    "                num_concurrency_post = np.zeros(n_rows)\n",
    "            else:\n",
    "                num_concurrency_post = rows[\"num_concurrent_queries\"].values - rows[\"num_concurrent_queries_train\"].values\n",
    "            global_num_concurrency_post.append(num_concurrency_post)\n",
    "\n",
    "        global_y = np.concatenate(global_y)\n",
    "        global_isolated_runtime = np.concatenate(global_isolated_runtime)\n",
    "        global_avg_runtime = np.concatenate(global_avg_runtime)\n",
    "        global_num_concurrency_pre = np.concatenate(global_num_concurrency_pre)\n",
    "        global_num_concurrency_post = np.concatenate(global_num_concurrency_post)\n",
    "\n",
    "        global_est_scan = np.concatenate(global_est_scan)\n",
    "        global_max_est_card = np.concatenate(global_max_est_card)\n",
    "        global_avg_est_card = np.concatenate(global_avg_est_card)\n",
    "        global_avg_time_elapsed_pre = np.asarray(global_avg_time_elapsed_pre)\n",
    "        global_sum_time_overlap_post = np.asarray(global_sum_time_overlap_post)\n",
    "        global_sum_concurrent_runtime_pre = np.asarray(global_sum_concurrent_runtime_pre)\n",
    "        global_sum_concurrent_runtime_post = np.asarray(global_sum_concurrent_runtime_post)\n",
    "        global_est_concurrent_scan_pre = np.asarray(global_est_concurrent_scan_pre)\n",
    "        global_est_concurrent_scan_post = np.asarray(global_est_concurrent_scan_post)\n",
    "        global_max_concurrent_card_pre = np.asarray(global_max_concurrent_card_pre)\n",
    "        global_max_concurrent_card_post = np.asarray(global_max_concurrent_card_post)\n",
    "        global_avg_concurrent_card_pre = np.asarray(global_avg_concurrent_card_pre)\n",
    "        global_avg_concurrent_card_post = np.asarray(global_avg_concurrent_card_post)\n",
    "        global_scan_sharing_percentage = np.asarray(global_scan_sharing_percentage)\n",
    "        feature = (\n",
    "            global_isolated_runtime,\n",
    "            global_avg_runtime,\n",
    "            global_num_concurrency_pre,\n",
    "            global_num_concurrency_post,\n",
    "            global_sum_concurrent_runtime_pre,\n",
    "            global_sum_concurrent_runtime_post,\n",
    "            global_avg_time_elapsed_pre,\n",
    "            global_sum_time_overlap_post,\n",
    "            global_est_scan,\n",
    "            global_est_concurrent_scan_pre,\n",
    "            global_est_concurrent_scan_post,\n",
    "            global_scan_sharing_percentage,\n",
    "            global_max_est_card,\n",
    "            global_avg_est_card,\n",
    "            global_max_concurrent_card_pre,\n",
    "            global_max_concurrent_card_post,\n",
    "            global_avg_concurrent_card_pre,\n",
    "            global_avg_concurrent_card_post\n",
    "        )\n",
    "        if self.opt_method == \"torch\" or self.opt_method == \"nn\":\n",
    "            feature = list(feature)\n",
    "            for i in range(len(feature)):\n",
    "                feature[i] = torch.from_numpy(feature[i])\n",
    "            feature = tuple(feature)\n",
    "            global_y = torch.from_numpy(global_y)\n",
    "        return feature, global_y, global_query_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d03f0-e4fd-492c-ba0c-38dcade0d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_separation_func_scipy(\n",
    "    x,\n",
    "    n1,\n",
    "    q1,\n",
    "    i1,\n",
    "    i2,\n",
    "    c1,\n",
    "    c2,\n",
    "    m1,\n",
    "    m2,\n",
    "    m3,\n",
    "    m4,\n",
    "    m5,\n",
    "    cm1,\n",
    "    r1,\n",
    "    r2,\n",
    "    max_concurrency,\n",
    "    avg_io_speed,\n",
    "    memory_size,\n",
    "):\n",
    "    \"\"\"\n",
    "    An analytical function that can consider 3 types of resource sharing/contention: IO, memory, CPU\n",
    "    x:: input tuple containing:\n",
    "        isolated_runtime: the isolated runtime without concurrency of a query\n",
    "        avg_runtime: average or median observed runtime of a query under any concurrency\n",
    "        num_concurrency: number of concurrent queries running with this query\n",
    "        sum_concurrent_runtime: sum of the estimated runtime of all queries concurrently running with this query (CPU)\n",
    "        est_scan: estimated MB of data that this query will need to scan (IO)\n",
    "        est_concurrent_scan: estimated MB of data that the concurrently running queries will need to scan (IO)\n",
    "        scan_sharing_percentage: estimated percentage of data in cache (sharing) according to concurrent queries\n",
    "        max_est_card: maximum estimated cardinality in the query plan of this query (reflect peak memory usage)\n",
    "        avg_est_card: average estimated cardinality in the query plan of this query (reflect average memory usage)\n",
    "        max_concurrent_card: maximum estimated cardinality for all concurrent queries\n",
    "        avg_concurrent_card: average estimated cardinality for all concurrent queries\n",
    "    TODO: adding memory, vCPU, and bandwidth information\n",
    "    \"\"\"\n",
    "    (\n",
    "        isolated_runtime,\n",
    "        avg_runtime,\n",
    "        num_concurrency_pre,\n",
    "        num_concurrency_post,\n",
    "        sum_concurrent_runtime_pre,\n",
    "        sum_concurrent_runtime_post,\n",
    "        avg_time_elapsed_pre,\n",
    "        sum_time_overlap_post,\n",
    "        est_scan,\n",
    "        est_concurrent_scan_pre,\n",
    "        est_concurrent_scan_post,\n",
    "        scan_sharing_percentage,\n",
    "        max_est_card,\n",
    "        avg_est_card,\n",
    "        max_concurrent_card_pre,\n",
    "        max_concurrent_card_post,\n",
    "        avg_concurrent_card_pre,\n",
    "        avg_concurrent_card_post\n",
    "    ) = x\n",
    "    # fraction of running queries (as opposed to queueing queries)\n",
    "    print(\"==============================================================================\")\n",
    "    running_frac = np.minimum(num_concurrency_pre + num_concurrency_post, max_concurrency) / np.maximum(\n",
    "        num_concurrency_pre + num_concurrency_post, 1\n",
    "    )\n",
    "    # estimate queueing time of a query based on the sum of concurrent queries' run time\n",
    "    queueing_time = (\n",
    "        q1\n",
    "        * (\n",
    "            np.maximum(num_concurrency_pre + n1 * num_concurrency_post - max_concurrency, 0)\n",
    "            / np.maximum(num_concurrency_pre + n1 * num_concurrency_post, 1)\n",
    "        )\n",
    "        *\n",
    "        (sum_concurrent_runtime_pre + n1 * sum_concurrent_runtime_post - avg_time_elapsed_pre * num_concurrency_pre)\n",
    "    )\n",
    "    queueing_time = np.maximum(queueing_time, 0)\n",
    "    discount_pre = (sum_concurrent_runtime_pre - avg_time_elapsed_pre * num_concurrency_pre) * running_frac / np.maximum(sum_concurrent_runtime_pre, 0.1)\n",
    "    discount_pre = np.maximum(discount_pre, 0)\n",
    "    discount_post = sum_time_overlap_post / np.maximum(sum_concurrent_runtime_post, 0.1)\n",
    "    # estimate io_speed of a query assuming each query has a base io_speed of i1 + the io speed due to contention\n",
    "    io_speed = i1 + avg_io_speed / np.minimum(\n",
    "        np.maximum(num_concurrency_pre * discount_pre + n1 * num_concurrency_post * discount_post, 1), max_concurrency\n",
    "    )\n",
    "    # estimate time speed on IO as the (estimated scan - data in cache) / estimated io_speed\n",
    "    # use i2 to adjust the estimation error in est_scan and scan_sharing_percentage\n",
    "    io_time = i2 * est_scan * (1 - scan_sharing_percentage * running_frac) / io_speed\n",
    "    # estimate the amount of CPU work/time as the weighted average of isolated_runtime and avg_runtime - io_time\n",
    "    cpu_time_isolated = np.maximum((r1 * isolated_runtime + r2 * avg_runtime) - io_time, 0.1)\n",
    "    # estimate the amount of CPU work imposed by the concurrent queries (approximated by their estimate runtime)\n",
    "    cpu_concurrent_pre = (sum_concurrent_runtime_pre * discount_pre) / avg_runtime\n",
    "    cpu_concurrent_post = sum_time_overlap_post / avg_runtime\n",
    "    \n",
    "    # estimate the amount of memory load imposed by the concurrent queries\n",
    "    max_mem_usage_perc_pre = max_est_card / (max_concurrent_card_pre + max_est_card)\n",
    "    avg_mem_usage_perc_pre = avg_est_card / (avg_concurrent_card_pre + avg_est_card)\n",
    "    max_mem_usage_perc_post = max_est_card / (max_concurrent_card_post + max_est_card)\n",
    "    avg_mem_usage_perc_post = avg_est_card / (avg_concurrent_card_post + avg_est_card)\n",
    "    peak_mem_usage = (m1 * np.maximum(max_concurrent_card_pre + max_est_card - memory_size, 0.01) * max_mem_usage_perc_pre + m1\n",
    "        * np.maximum(max_concurrent_card_post + max_est_card - memory_size, 0.01)\n",
    "        * max_mem_usage_perc_post\n",
    "        ) / memory_size\n",
    "    avg_mem_usage = (m2 * np.maximum(avg_concurrent_card_pre + avg_est_card - memory_size, 0.01)\n",
    "        * avg_mem_usage_perc_pre + m2\n",
    "        * np.maximum(avg_concurrent_card_post + avg_est_card - memory_size, 0.01)\n",
    "        * avg_mem_usage_perc_post\n",
    "        ) / memory_size\n",
    "    mem_usage = m3 * peak_mem_usage + m4 * avg_mem_usage\n",
    "    # estimate the CPU time of a query by considering the contention of CPU and memory of other queries\n",
    "\n",
    "    cpu_time_scale_factor = (\n",
    "        c1 * (cpu_concurrent_pre + c2 * cpu_concurrent_post)\n",
    "    ) * (1 + m5 * mem_usage + cm1 * np.sqrt((cpu_concurrent_pre + cpu_concurrent_post) * mem_usage)) \n",
    "    cpu_time = (1 + cpu_time_scale_factor) * cpu_time_isolated\n",
    "    # final runtime of a query is estimated to be the queueing time + io_time + cpu_time\n",
    "    print(\"est_scan:\", np.min(est_scan), np.mean(est_scan), np.max(est_scan))\n",
    "    print(\"cpu_concurrent_pre:\", np.min(cpu_concurrent_pre), np.mean(cpu_concurrent_pre), np.max(cpu_concurrent_pre))\n",
    "    print(\"cpu_concurrent_post:\", np.min(cpu_concurrent_post), np.mean(cpu_concurrent_post), np.max(cpu_concurrent_post))\n",
    "    print(\"max_est_card:\", np.min(max_est_card), np.mean(max_est_card), np.max(max_est_card))\n",
    "    print(\"max_concurrent_card_pre:\", np.min(max_concurrent_card_pre), np.mean(max_concurrent_card_pre), np.max(max_concurrent_card_pre))\n",
    "    print(\"max_concurrent_card_post:\", np.min(max_concurrent_card_post), np.mean(max_concurrent_card_post), np.max(max_concurrent_card_post))\n",
    "    print(\"avg_est_card:\", np.min(avg_est_card), np.mean(avg_est_card), np.max(avg_est_card))\n",
    "    print(\"avg_concurrent_card_pre:\", np.min(avg_concurrent_card_pre), np.mean(avg_concurrent_card_pre), np.max(avg_concurrent_card_pre))\n",
    "    print(\"avg_concurrent_card_post:\", np.min(avg_concurrent_card_post), np.mean(avg_concurrent_card_post), np.max(avg_concurrent_card_post))\n",
    "    print(\"mem_usage:\", np.min(mem_usage), np.mean(mem_usage), np.max(mem_usage))\n",
    "    print(\"cpu_time_isolated\", np.min(cpu_time_isolated), np.mean(cpu_time_isolated),  np.max(cpu_time_isolated))\n",
    "    print(\"queueing time:\", np.min(queueing_time), np.mean(queueing_time), np.max(queueing_time))\n",
    "    print(\"io time:\", np.min(io_time), np.mean(io_time), np.max(io_time))\n",
    "    print(\"CPU time:\", np.min(cpu_time), np.mean(cpu_time), np.max(cpu_time))\n",
    "    return np.maximum(queueing_time + io_time + cpu_time, 0.01)\n",
    "\n",
    "\n",
    "def interaction_separation_func_scipy_archive(\n",
    "    x,\n",
    "    n1,\n",
    "    q1,\n",
    "    i1,\n",
    "    i2,\n",
    "    c1,\n",
    "    c2,\n",
    "    m1,\n",
    "    m2,\n",
    "    m3,\n",
    "    m4,\n",
    "    m5,\n",
    "    cm1,\n",
    "    r1,\n",
    "    r2,\n",
    "    max_concurrency,\n",
    "    avg_io_speed,\n",
    "    memory_size,\n",
    "):\n",
    "    \"\"\"\n",
    "    An analytical function that can consider 3 types of resource sharing/contention: IO, memory, CPU\n",
    "    x:: input tuple containing:\n",
    "        isolated_runtime: the isolated runtime without concurrency of a query\n",
    "        avg_runtime: average or median observed runtime of a query under any concurrency\n",
    "        num_concurrency: number of concurrent queries running with this query\n",
    "        sum_concurrent_runtime: sum of the estimated runtime of all queries concurrently running with this query (CPU)\n",
    "        est_scan: estimated MB of data that this query will need to scan (IO)\n",
    "        est_concurrent_scan: estimated MB of data that the concurrently running queries will need to scan (IO)\n",
    "        scan_sharing_percentage: estimated percentage of data in cache (sharing) according to concurrent queries\n",
    "        max_est_card: maximum estimated cardinality in the query plan of this query (reflect peak memory usage)\n",
    "        avg_est_card: average estimated cardinality in the query plan of this query (reflect average memory usage)\n",
    "        max_concurrent_card: maximum estimated cardinality for all concurrent queries\n",
    "        avg_concurrent_card: average estimated cardinality for all concurrent queries\n",
    "    TODO: adding memory and CPU information\n",
    "    \"\"\"\n",
    "    (\n",
    "        isolated_runtime,\n",
    "        avg_runtime,\n",
    "        num_concurrency_pre,\n",
    "        num_concurrency_post,\n",
    "        sum_concurrent_runtime_pre,\n",
    "        sum_concurrent_runtime_post,\n",
    "        avg_time_elapsed_pre,\n",
    "        sum_time_overlap_post,\n",
    "        est_scan,\n",
    "        est_concurrent_scan_pre,\n",
    "        est_concurrent_scan_post,\n",
    "        scan_sharing_percentage,\n",
    "        max_est_card,\n",
    "        avg_est_card,\n",
    "        max_concurrent_card_pre,\n",
    "        max_concurrent_card_post,\n",
    "        avg_concurrent_card_pre,\n",
    "        avg_concurrent_card_post\n",
    "    ) = x\n",
    "    # fraction of running queries (as opposed to queueing queries)\n",
    "    running_frac = np.minimum(num_concurrency_pre + num_concurrency_post, max_concurrency) / np.maximum(\n",
    "        num_concurrency_pre + num_concurrency_post, 1\n",
    "    )\n",
    "    print(\"==============================================================================\")\n",
    "    # estimate queueing time of a query based on the sum of concurrent queries' run time\n",
    "    queueing_time = (\n",
    "        q1\n",
    "        * (\n",
    "            np.maximum(num_concurrency_pre + n1 * num_concurrency_post - max_concurrency, 0)\n",
    "            / np.maximum(num_concurrency_pre + n1 * num_concurrency_post, 1)\n",
    "        )\n",
    "        *\n",
    "        (sum_concurrent_runtime_pre + n1 * sum_concurrent_runtime_post - avg_time_elapsed_pre * num_concurrency_pre)\n",
    "    )\n",
    "    queueing_time = np.maximum(queueing_time, 0)\n",
    "    discount_pre = (sum_concurrent_runtime_pre - avg_time_elapsed_pre * num_concurrency_pre) * running_frac / np.maximum(sum_concurrent_runtime_pre, 0.1)\n",
    "    discount_pre = np.maximum(discount_pre, 0)\n",
    "    discount_post = sum_time_overlap_post / np.maximum(sum_concurrent_runtime_post, 0.1)\n",
    "    # estimate io_speed of a query assuming each query has a base io_speed of i1 + the io speed due to contention\n",
    "    io_speed = i1 + avg_io_speed / np.minimum(\n",
    "        np.maximum(num_concurrency_pre * discount_pre + n1 * num_concurrency_post * discount_post, 1), max_concurrency\n",
    "    )\n",
    "    # estimate time speed on IO as the (estimated scan - data in cache) / estimated io_speed\n",
    "    # use i2 to adjust the estimation error in est_scan and scan_sharing_percentage\n",
    "    print(\"est_scan:\", np.min(est_scan), np.mean(est_scan), np.max(est_scan))\n",
    "    io_time = i2 * est_scan * (1 - scan_sharing_percentage * running_frac) / io_speed\n",
    "    # estimate the amount of CPU work/time as the weighted average of isolated_runtime and avg_runtime - io_time\n",
    "    cpu_time_isolated = np.maximum((r1 * isolated_runtime + r2 * avg_runtime) - io_time, 0.1)\n",
    "    # estimate the amount of CPU work imposed by the concurrent queries (approximated by their estimate runtime)\n",
    "    cpu_concurrent_pre = (sum_concurrent_runtime_pre * discount_pre) / np.maximum(avg_runtime, 2)\n",
    "    cpu_concurrent_post = sum_time_overlap_post / np.maximum(avg_runtime, 2)\n",
    "    print(\"cpu_concurrent_pre:\", np.min(cpu_concurrent_pre), np.mean(cpu_concurrent_pre), np.max(cpu_concurrent_pre))\n",
    "    print(\"cpu_concurrent_post:\", np.min(cpu_concurrent_post), np.mean(cpu_concurrent_post), np.max(cpu_concurrent_post))\n",
    "    # estimate the amount of memory load imposed by the concurrent queries\n",
    "    max_mem_usage_perc_pre = max_concurrent_card_pre / (max_concurrent_card_pre + max_est_card)\n",
    "    avg_mem_usage_perc_pre = avg_concurrent_card_pre / (avg_concurrent_card_pre + avg_est_card)\n",
    "    max_mem_usage_perc_post = max_concurrent_card_post / (max_concurrent_card_post + max_est_card)\n",
    "    avg_mem_usage_perc_post = avg_concurrent_card_post / (avg_concurrent_card_post + avg_est_card)\n",
    "    print(\"max_est_card:\", np.min(max_est_card), np.mean(max_est_card), np.max(max_est_card))\n",
    "    print(\"max_concurrent_card_pre:\", np.min(max_concurrent_card_pre), np.mean(max_concurrent_card_pre), np.max(max_concurrent_card_pre))\n",
    "    print(\"max_concurrent_card_post:\", np.min(max_concurrent_card_post), np.mean(max_concurrent_card_post), np.max(max_concurrent_card_post))\n",
    "    print(\"avg_est_card:\", np.min(avg_est_card), np.mean(avg_est_card), np.max(avg_est_card))\n",
    "    print(\"avg_concurrent_card_pre:\", np.min(avg_concurrent_card_pre), np.mean(avg_concurrent_card_pre), np.max(avg_concurrent_card_pre))\n",
    "    print(\"avg_concurrent_card_post:\", np.min(avg_concurrent_card_post), np.mean(avg_concurrent_card_post), np.max(avg_concurrent_card_post))\n",
    "    memory_concurrent = np.log(\n",
    "        m1\n",
    "        * np.maximum(max_concurrent_card_pre + max_est_card - memory_size, 0.01)\n",
    "        * max_mem_usage_perc_pre\n",
    "        + m2\n",
    "        * np.maximum(avg_concurrent_card_pre + avg_est_card - memory_size, 0.01)\n",
    "        * avg_mem_usage_perc_pre\n",
    "        + m3\n",
    "        * np.maximum(max_concurrent_card_post + max_est_card - memory_size, 0.01)\n",
    "        * max_mem_usage_perc_post\n",
    "        + m4\n",
    "        * np.maximum(avg_concurrent_card_post + avg_est_card - memory_size, 0.01)\n",
    "        * avg_mem_usage_perc_post\n",
    "        + 0.0001\n",
    "    ) * np.log((m1 + m3) * max_est_card + (m2 + m4) * avg_est_card + 0.0001)\n",
    "    memory_concurrent = np.maximum(memory_concurrent, 0)\n",
    "    print(\"memory_concurrent:\", np.min(memory_concurrent), np.mean(memory_concurrent), np.max(memory_concurrent))\n",
    "    print(\"cpu_time_isolated\", np.min(cpu_time_isolated), np.mean(cpu_time_isolated),  np.max(cpu_time_isolated))\n",
    "    memory_concurrent = np.maximum(memory_concurrent, 0)\n",
    "    # estimate the CPU time of a query by considering the contention of CPU and memory of other queries\n",
    "    cpu_time = (\n",
    "        1\n",
    "        + c1 * cpu_concurrent_pre\n",
    "        + c1 * cpu_concurrent_post\n",
    "        + m5 * memory_concurrent\n",
    "        + cm1 * np.sqrt((cpu_concurrent_pre + cpu_concurrent_post) * memory_concurrent)\n",
    "    ) * cpu_time_isolated\n",
    "    # final runtime of a query is estimated to be the queueing time + io_time + cpu_time\n",
    "    print(\"queueing time:\", np.min(queueing_time), np.mean(queueing_time), np.max(queueing_time))\n",
    "    print(\"io time:\", np.min(io_time), np.mean(io_time), np.max(io_time))\n",
    "    print(\"CPU time:\", np.min(cpu_time), np.mean(cpu_time), np.max(cpu_time))\n",
    "    return np.maximum(queueing_time + io_time + cpu_time, 0.01)\n",
    "    \n",
    "\n",
    "class ComplexFitCurveSeparation(ComplexFitCurve):\n",
    "    \"\"\"\n",
    "    Complex fit curve model for runtime prediction with concurrency\n",
    "    See interaction_func_scipy for detailed analytical functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_column_store=False, opt_method='scipy'):\n",
    "        super().__init__(is_column_store, opt_method)\n",
    "        self.analytic_params = [0.3, 0.5, 20, 2, 0.2, 0.9, 0.3, 0.3, 0.3, 0.3, 0.1, 0.2, 0.5, 0.5, 10, 200, 16000]\n",
    "        self.param_names = ['n1', 'q1', 'i1', 'i2', 'c1', 'c2', 'm1', 'm2', 'm3', 'm4', 'm5', 'cm1', \n",
    "                            'r1', 'r2', 'max_concurrency', 'avg_io_speed', 'memory_size',]\n",
    "        self.bound = optimization.Bounds(\n",
    "            [0.01, 0.1, 10, 0.1, 0.0001, 0.0001, 0.0001, 0.01, 0.0001, 0.01, 0.01, 0.01, 0.4, 0.4, 10, 20, 10000],\n",
    "            [1, 1, 200, 2, 1, 1, 1, 0.9, 0.5, 0.5, 0.5, 0.5, 0.8, 0.8, 20, 2000, 50000],\n",
    "        )\n",
    "        self.constrain = optimization.Bounds(\n",
    "            [0.1, 0.1, 10, 0.1, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.5, 0.05, 2, 20, 10000],\n",
    "            [1, 1, 200, 2, 1, 0.9, 0.9, 0.9, 0.5, 0.5, 0.5, 0.5, 0.95, 0.4, 20, 2000, 50000],\n",
    "        )\n",
    "        self.penalty = [100, 100, 0.1, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 1, 0.1, 0.01]\n",
    "\n",
    "    def featurize_data(self, concurrent_df):\n",
    "        global_y = []\n",
    "        global_isolated_runtime = []\n",
    "        global_avg_runtime = []\n",
    "        global_num_concurrency_pre = []\n",
    "        global_num_concurrency_post = []\n",
    "        global_sum_concurrent_runtime_pre = []\n",
    "        global_sum_concurrent_runtime_post = []\n",
    "        global_avg_time_elapsed_pre = []\n",
    "        global_sum_time_overlap_post = []\n",
    "        global_est_scan = []\n",
    "        global_est_concurrent_scan_pre = []\n",
    "        global_est_concurrent_scan_post = []\n",
    "        global_scan_sharing_percentage = []\n",
    "        global_max_est_card = []\n",
    "        global_avg_est_card = []\n",
    "        global_max_concurrent_card_pre = []\n",
    "        global_max_concurrent_card_post = []\n",
    "        global_avg_concurrent_card_pre = []\n",
    "        global_avg_concurrent_card_post = []\n",
    "        global_query_idx = dict()\n",
    "        start = 0\n",
    "        for i, rows in concurrent_df.groupby(\"query_idx\"):\n",
    "            if (\n",
    "                    i not in self.isolated_rt_cache\n",
    "                    or i not in self.query_info\n",
    "                    or i not in self.average_rt_cache\n",
    "            ):\n",
    "                continue\n",
    "            concurrent_rt = rows[\"runtime\"].values\n",
    "            start_time = rows[\"start_time\"].values\n",
    "            end_time = rows[\"end_time\"].values\n",
    "            query_info = self.query_info[i]\n",
    "            n_rows = len(rows)\n",
    "            num_concurrency_pre = rows[\"num_concurrent_queries_train\"].values\n",
    "            global_num_concurrency_pre.append(num_concurrency_pre)\n",
    "            concur_info_prev = rows[\"concur_info_train\"].values\n",
    "            full_concur_info = rows[\"concur_info\"].values\n",
    "            concur_info_post = []\n",
    "            for j in range(len(full_concur_info)):\n",
    "                new_info = [c for c in full_concur_info[j] if c not in full_concur_info[j]]\n",
    "                concur_info_post.append(new_info)\n",
    "            pre_exec_info = rows[\"pre_exec_info\"].values\n",
    "\n",
    "            global_query_idx[i] = (start, start + n_rows)\n",
    "            start += n_rows\n",
    "            global_y.append(concurrent_rt)\n",
    "            global_isolated_runtime.append(np.ones(n_rows) * self.isolated_rt_cache[i])\n",
    "            global_avg_runtime.append(np.ones(n_rows) * self.average_rt_cache[i])\n",
    "            global_est_scan.append(np.ones(n_rows) * query_info[\"est_scan\"])\n",
    "            global_max_est_card.append(\n",
    "                np.ones(n_rows) * np.max(query_info[\"all_cardinality\"]) / (1024 * 1024)\n",
    "            )\n",
    "            global_avg_est_card.append(\n",
    "                np.ones(n_rows)\n",
    "                * np.average(query_info[\"all_cardinality\"])\n",
    "                / (1024 * 1024)\n",
    "            )\n",
    "            for j in range(n_rows):\n",
    "                sum_concurrent_runtime_pre = 0\n",
    "                sum_concurrent_runtime_post = 0\n",
    "                sum_concurrent_scan_pre = 0\n",
    "                sum_concurrent_scan_post = 0\n",
    "                avg_time_elapsed_pre = 0\n",
    "                sum_time_overlap_post = 0\n",
    "                concurrent_card_pre = []\n",
    "                concurrent_card_post = []\n",
    "                for c in full_concur_info[j]:\n",
    "                    if c[0] in self.average_rt_cache:\n",
    "                        if c in concur_info_prev[j]:\n",
    "                            sum_concurrent_runtime_pre += self.average_rt_cache[c[0]]\n",
    "                            avg_time_elapsed_pre += (start_time[j] - c[1])\n",
    "                        else:\n",
    "                            sum_concurrent_runtime_post += self.average_rt_cache[c[0]]\n",
    "                            # TODO: this is not practical, make it an estimation\n",
    "                            sum_time_overlap_post += (end_time[j] - c[1])\n",
    "                    else:\n",
    "                        print(c[0])\n",
    "                    if c[0] in self.query_info:\n",
    "                        if c in concur_info_prev[j]:\n",
    "                            sum_concurrent_scan_pre += self.query_info[c[0]][\"est_scan\"]\n",
    "                            concurrent_card_pre.extend(self.query_info[c[0]][\"all_cardinality\"])\n",
    "                        else:\n",
    "                            sum_concurrent_scan_post += self.query_info[c[0]][\"est_scan\"]\n",
    "                            concurrent_card_post.extend(self.query_info[c[0]][\"all_cardinality\"])\n",
    "                    else:\n",
    "                        print(c[0])\n",
    "\n",
    "                global_sum_concurrent_runtime_pre.append(sum_concurrent_runtime_pre)\n",
    "                global_avg_time_elapsed_pre.append(avg_time_elapsed_pre / (len(concur_info_prev[j]) + 0.001))\n",
    "                global_est_concurrent_scan_pre.append(sum_concurrent_scan_pre)\n",
    "                if len(concurrent_card_pre) == 0:\n",
    "                    global_max_concurrent_card_pre.append(0)\n",
    "                    global_avg_concurrent_card_pre.append(0)\n",
    "                else:\n",
    "                    global_max_concurrent_card_pre.append(\n",
    "                        np.max(concurrent_card_pre) / (1024 * 1024)\n",
    "                    )\n",
    "                    global_avg_concurrent_card_pre.append(\n",
    "                        np.average(concurrent_card_pre) / (1024 * 1024)\n",
    "                    )\n",
    "                # TODO: may be able to change concur_info_prev to full_concur_info?\n",
    "                global_scan_sharing_percentage.append(\n",
    "                    self.estimate_data_share_percentage(\n",
    "                        i, concur_info_prev[j], pre_exec_info[j]\n",
    "                    )\n",
    "                )\n",
    "                if self.use_pre_info:\n",
    "                    global_sum_concurrent_runtime_post.append(0)\n",
    "                    global_est_concurrent_scan_post.append(0)\n",
    "                    global_sum_time_overlap_post.append(0)\n",
    "                    global_max_concurrent_card_post.append(0)\n",
    "                    global_avg_concurrent_card_post.append(0)\n",
    "                else:\n",
    "                    global_sum_concurrent_runtime_post.append(sum_concurrent_runtime_post)\n",
    "                    global_est_concurrent_scan_post.append(sum_concurrent_scan_post)\n",
    "                    global_sum_time_overlap_post.append(sum_time_overlap_post)\n",
    "                    if len(concurrent_card_post) == 0:\n",
    "                        global_max_concurrent_card_post.append(0)\n",
    "                        global_avg_concurrent_card_post.append(0)\n",
    "                    else:\n",
    "                        global_max_concurrent_card_post.append(\n",
    "                            np.max(concurrent_card_post) / (1024 * 1024)\n",
    "                        )\n",
    "                        global_avg_concurrent_card_post.append(\n",
    "                            np.average(concurrent_card_post) / (1024 * 1024)\n",
    "                        )\n",
    "\n",
    "            if self.use_pre_info:\n",
    "                num_concurrency_post = np.zeros(n_rows)\n",
    "            else:\n",
    "                num_concurrency_post = rows[\"num_concurrent_queries\"].values - rows[\"num_concurrent_queries_train\"].values\n",
    "            global_num_concurrency_post.append(num_concurrency_post)\n",
    "\n",
    "        global_y = np.concatenate(global_y)\n",
    "        global_isolated_runtime = np.concatenate(global_isolated_runtime)\n",
    "        global_avg_runtime = np.concatenate(global_avg_runtime)\n",
    "        global_num_concurrency_pre = np.concatenate(global_num_concurrency_pre)\n",
    "        global_num_concurrency_post = np.concatenate(global_num_concurrency_post)\n",
    "\n",
    "        global_est_scan = np.concatenate(global_est_scan)\n",
    "        global_max_est_card = np.concatenate(global_max_est_card)\n",
    "        global_avg_est_card = np.concatenate(global_avg_est_card)\n",
    "        global_avg_time_elapsed_pre = np.asarray(global_avg_time_elapsed_pre)\n",
    "        global_sum_time_overlap_post = np.asarray(global_sum_time_overlap_post)\n",
    "        global_sum_concurrent_runtime_pre = np.asarray(global_sum_concurrent_runtime_pre)\n",
    "        global_sum_concurrent_runtime_post = np.asarray(global_sum_concurrent_runtime_post)\n",
    "        global_est_concurrent_scan_pre = np.asarray(global_est_concurrent_scan_pre)\n",
    "        global_est_concurrent_scan_post = np.asarray(global_est_concurrent_scan_post)\n",
    "        global_max_concurrent_card_pre = np.asarray(global_max_concurrent_card_pre)\n",
    "        global_max_concurrent_card_post = np.asarray(global_max_concurrent_card_post)\n",
    "        global_avg_concurrent_card_pre = np.asarray(global_avg_concurrent_card_pre)\n",
    "        global_avg_concurrent_card_post = np.asarray(global_avg_concurrent_card_post)\n",
    "        global_scan_sharing_percentage = np.asarray(global_scan_sharing_percentage)\n",
    "        feature = (\n",
    "            global_isolated_runtime,\n",
    "            global_avg_runtime,\n",
    "            global_num_concurrency_pre,\n",
    "            global_num_concurrency_post,\n",
    "            global_sum_concurrent_runtime_pre,\n",
    "            global_sum_concurrent_runtime_post,\n",
    "            global_avg_time_elapsed_pre,\n",
    "            global_sum_time_overlap_post,\n",
    "            global_est_scan,\n",
    "            global_est_concurrent_scan_pre,\n",
    "            global_est_concurrent_scan_post,\n",
    "            global_scan_sharing_percentage,\n",
    "            global_max_est_card,\n",
    "            global_avg_est_card,\n",
    "            global_max_concurrent_card_pre,\n",
    "            global_max_concurrent_card_post,\n",
    "            global_avg_concurrent_card_pre,\n",
    "            global_avg_concurrent_card_post\n",
    "        )\n",
    "        if self.opt_method == \"torch\" or self.opt_method == \"nn\":\n",
    "            feature = list(feature)\n",
    "            for i in range(len(feature)):\n",
    "                feature[i] = torch.from_numpy(feature[i])\n",
    "            feature = tuple(feature)\n",
    "            global_y = torch.from_numpy(global_y)\n",
    "        return feature, global_y, global_query_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bb812-bcd0-4d66-924d-7bfb1793501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_queries_path = \"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_aurora/aurora_mixed_parsed_queries.json\"\n",
    "cfc = ComplexFitCurveSeparation(opt_method='scipy')\n",
    "cfc.pre_process_queries(parsed_queries_path)\n",
    "cfc.train(concurrency_df.iloc[train_idx], use_train=False, isolated_trace_df=isolated_trace_df, analytic_func=interaction_separation_func_scipy)\n",
    "predictions_cfc, labels = cfc.predict(eval_trace_df)\n",
    "print(\"===========Performance for simple linear regression model (all query)=============\")\n",
    "result_overall_cfc, result_per_query_cfc = cfc.evaluate_performance(eval_trace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4e998-afa9-4979-945a-b5614db42c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(cfc.param_names, cfc.analytic_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30cc898-f600-4197-a142-7bac29537138",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_queries_path = \"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_aurora/aurora_mixed_parsed_queries.json\"\n",
    "cfc = ComplexFitCurve()\n",
    "cfc.pre_process_queries(parsed_queries_path)\n",
    "cfc.train(concurrency_df.iloc[train_idx], isolated_trace_df=isolated_trace_df)\n",
    "predictions_cfc, labels = cfc.predict(eval_trace_df, use_global=True)\n",
    "print(\"===========Performance for simple linear regression model (all query)=============\")\n",
    "result_overall_cfc, result_per_query_cfc = cfc.evaluate_performance(concurrency_df.iloc[train_idx], use_global=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1e54df-ca83-48ac-bba9-ebaf363216a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ftol` termination condition is satisfied.\n",
      "Function evaluations 21, initial cost 2.5050e+06, final cost 1.0646e+06, first-order optimality 3.07e+00.\n",
      "===========Performance for simple linear regression model (all query)=============\n",
      "50% absolute error is 4.037889813700423, q-error is 1.8066403080837505\n",
      "90% absolute error is 75.8822444896508, q-error is 7.071717262372014\n",
      "95% absolute error is 118.92453050874809, q-error is 12.348696329207725\n"
     ]
    }
   ],
   "source": [
    "parsed_queries_path = \"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_aurora/aurora_mixed_parsed_queries.json\"\n",
    "cfc = ComplexFitCurve(opt_method='scipy')\n",
    "cfc.pre_process_queries(parsed_queries_path)\n",
    "cfc.train(concurrency_df.iloc[train_idx], use_train=False, isolated_trace_df=isolated_trace_df)\n",
    "predictions_cfc, labels = cfc.predict(eval_trace_df)\n",
    "print(\"===========Performance for simple linear regression model (all query)=============\")\n",
    "result_overall_cfc, result_per_query_cfc = cfc.evaluate_performance(eval_trace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c49d4ca5-7a82-4810-b975-1bc4e7217877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([43579, 11])\n",
      "0 10609958.174529513\n",
      "50 2538246.7744387286\n",
      "100 2732234.654581448\n",
      "150 1538667.552942489\n",
      "200 2015665.4563286707\n",
      "250 2886854.3740503374\n",
      "300 1839310.7574020748\n",
      "350 1875102.2930603507\n",
      "400 2301317.7323573837\n",
      "450 1442929.0626926236\n",
      "500 3226691.8574170694\n",
      "550 2238916.150878676\n",
      "600 2188140.748555953\n",
      "650 2120070.465729918\n",
      "700 1989791.5540937516\n",
      "750 1365114.917391506\n",
      "===========Performance for simple linear regression model (all query)=============\n",
      "50% absolute error is 12.399064302444458, q-error is 2.8025900842173708\n",
      "90% absolute error is 77.72694287300108, q-error is 31.913985463466727\n",
      "95% absolute error is 122.46469709873189, q-error is 90.18004941334297\n"
     ]
    }
   ],
   "source": [
    "parsed_queries_path = \"/Users/ziniuw/Desktop/research/Data/AWS_trace/mixed_aurora/aurora_mixed_parsed_queries.json\"\n",
    "cfc = ComplexFitCurve(opt_method='torch')\n",
    "cfc.pre_process_queries(parsed_queries_path)\n",
    "cfc.train(concurrency_df.iloc[train_idx], use_train=False, isolated_trace_df=isolated_trace_df)\n",
    "predictions_cfc, labels = cfc.predict(eval_trace_df)\n",
    "print(\"===========Performance for simple linear regression model (all query)=============\")\n",
    "result_overall_cfc, result_per_query_cfc = cfc.evaluate_performance(eval_trace_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151ebdd-3e50-4b1d-b45e-5d47f8d3d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfc.analytic_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cca09-e256-4630-8eb1-312c38f8dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_per_query_cfc:\n",
    "    print(i, result_per_query_cfc[i][0], result_per_query_cfc[i][1], result_per_query_sfc[i][0], result_per_query_sfc[i][1],\n",
    "          result_per_query_xgb[i][0], result_per_query_xgb[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011ffc49-3f08-4401-ada2-90c792c8c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.876003980636597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.25762579,   1.82775274,   0.001     ,   1.87200832],\n",
       "       [  2.26638432,   9.42341111,  13.14544487,   4.42703104],\n",
       "       [  2.31785227,  35.81023087,  13.1929636 , 114.38352728],\n",
       "       [  2.34516126,   9.42341111,  11.69839573,   4.58424425],\n",
       "       [  2.37033695,   9.42341111,   3.08058619,   5.95580792],\n",
       "       [  2.39154091,   1.43626839,   2.65020823,   1.93393612],\n",
       "       [  2.39552303,   9.42341111,   8.57998371,   2.32683921],\n",
       "       [  2.40113348,   9.42341111,  13.08828449,  88.36611104],\n",
       "       [  2.51346057,   9.42341111,  13.78222752,   4.91702175],\n",
       "       [  2.51785916,  22.61682099,  24.97361374,   2.35669684],\n",
       "       [  2.53823693,   1.43626839,   5.99422979,   2.1751008 ],\n",
       "       [  2.54637487,  22.61682099,   7.57959986,  10.10447383],\n",
       "       [  2.56652423,   1.43626839,   0.001     ,   4.40651274],\n",
       "       [  2.61211558,  22.61682099,   0.001     ,   2.61325145],\n",
       "       [  2.61289406,  22.61682099,  16.22226715,  31.38883948],\n",
       "       [  2.63804725,   9.42341111,  10.49375153,   2.01411271],\n",
       "       [  2.64168543,   9.42341111,  14.93830967,   2.96240592],\n",
       "       [  2.64399055,   9.42341111,   4.99606943,  12.52246213],\n",
       "       [  2.67370867,  22.61682099,  13.5591011 ,   4.46496224],\n",
       "       [  2.68042839,   9.42341111,  21.86740685,  11.28278399],\n",
       "       [  2.69408116,   9.42341111,   1.76727915,   2.21469569],\n",
       "       [  2.69808439,   9.42341111,   1.81146097,   3.78994226],\n",
       "       [  2.70432942,   9.42341111,   0.001     ,   4.75883269],\n",
       "       [  2.70805512,   9.42341111,  11.86859798,   2.04218674],\n",
       "       [  2.7209352 ,   1.43626839,   2.15051961,   2.42659426],\n",
       "       [  2.77418937,   9.42341111,  50.96062088,   2.83189964],\n",
       "       [  2.84596283,  22.61682099,  12.16388512,  11.17201853],\n",
       "       [  2.86564847,   9.42341111,  22.1607666 ,   4.06197071],\n",
       "       [  2.8961565 ,   9.42341111,   0.001     ,   2.38745499],\n",
       "       [  2.91656437,   9.42341111,   5.85018778,   2.23882627],\n",
       "       [  3.07536467,  35.81023087,  10.33505917,   4.2171936 ],\n",
       "       [  3.0969723 ,   9.42341111,   0.68501025,   1.96598458],\n",
       "       [  3.09851787,  49.00364075,  30.19099998,  18.17922497],\n",
       "       [  3.11287204,   9.42341111,   5.82118988,   2.88192558],\n",
       "       [  3.12955808,   9.42341111,   6.95914412,   4.05099583],\n",
       "       [  3.21097518,   9.42341111,   0.001     ,   2.6309731 ],\n",
       "       [  3.21294656,   9.42341111,  13.2132206 ,   2.51060104],\n",
       "       [  3.25702157,   9.42341111,  16.17326546,   9.16643   ],\n",
       "       [  3.3575131 ,   9.42341111,   0.10482162,   2.1152246 ],\n",
       "       [  4.14482264,   9.42341111,  18.74451447,   1.94137239],\n",
       "       [  4.16746008,  49.00364075,  14.48124695,  10.63966942],\n",
       "       [  4.82245152,   9.42341111,  17.63269043,  15.32919788],\n",
       "       [ 24.05566205,  62.19705063,  22.89256287,  46.13621497]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 7\n",
    "print(isolated_trace_df[\"runtime\"].iloc[i])\n",
    "idx = np.argsort(predictions_cfc[i])\n",
    "np.stack((predictions_cfc[i][idx], predictions_sfc[i][idx], predictions_xgb[i][idx], labels[i][idx]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfeaaa-7e86-4a4d-82a0-f49d2cb593ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07224cb7-e676-461a-a49f-0c59c389d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72a633-e8ed-479a-a85e-83b49e6b0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan[\"parsed_queries\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ab464-9d5e-41e2-818f-b789154d312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255382cb-7340-421a-85a1-3507627c66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.maximum(np.zeros(3), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81e9aee-f78d-4f9a-b98c-b7000855886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Performance for simple curve fitting model (all query)=============\n",
      "50% absolute error is 9.788035813636878, q-error is 2.3922804101475075\n",
      "90% absolute error is 73.57756001053527, q-error is 16.778080334168294\n",
      "95% absolute error is 119.07045884474977, q-error is 42.23310157942017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ziniuw/.local/lib/python3.11/site-packages/scipy/optimize/_minpack_py.py:1010: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n"
     ]
    }
   ],
   "source": [
    "sfc = SimpleFitCurve()\n",
    "sfc.train(concurrency_df.iloc[train_idx], use_train=False, isolated_trace_df=isolated_trace_df)\n",
    "#predictions, labels = sfc.predict(eval_trace_df)\n",
    "#print(\"===========Performance for simple curve fitting model (per query)=============\")\n",
    "#result_overall, result_per_query = sfc.evaluate_performance(eval_trace_df, use_train=True)\n",
    "predictions_sfc, labels = sfc.predict(eval_trace_df, use_global=True)\n",
    "print(\"===========Performance for simple curve fitting model (all query)=============\")\n",
    "result_overall_sfc, result_per_query_sfc = sfc.evaluate_performance(eval_trace_df, use_global=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd0ee3d-0783-4f1c-b9c6-bb106e1b0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Performance for XGBoost model (train on full)=============\n",
      "50% absolute error is 10.129798173904419, q-error is 2.6069710407156554\n",
      "90% absolute error is 65.102055311203, q-error is 449.9643588921217\n",
      "95% absolute error is 97.84108705520629, q-error is 1548.559878229542\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBoostPredictor(k=240)\n",
    "xgb.train(concurrency_df.iloc[train_idx], use_train=False, isolated_trace_df=isolated_trace_df, use_pre_exec_info=True)\n",
    "predictions_xgb, labels = xgb.predict(eval_trace_df)\n",
    "#predictions_xgb, labels = xgb.predict(concurrency_df.iloc[train_idx], use_train=False)\n",
    "print(\"===========Performance for XGBoost model (train on full)=============\")\n",
    "result_overall_xgb, result_per_query_xgb = xgb.evaluate_performance(eval_trace_df)\n",
    "#result_overall_xgb, result_per_query_xgb = xgb.evaluate_performance(concurrency_df.iloc[train_idx], use_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00fa37-c1cd-48bf-ad01-87e8549bc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2ca25-49ba-40ef-b3a7-3b2469d9e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(x, a, b):\n",
    "    return x * a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c14a5-d91f-4cc3-8eec-f1bbdd8f00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (np.ones(3), np.ones(3)+1, np.ones(3)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23c395-0eaa-4f49-b38b-77cb58948ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in a:\n",
    "    b = torch.from_numpy(b)\n",
    "    print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba0372-c2ec-446c-a206-97ea05836647",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (torch.ones(5), torch.ones(5), torch.ones(5))\n",
    "torch.stack(a, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b042f-a495-44ec-a7fd-b1bd2bed9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1 = torch.tensor(self.q1, requires_grad=True)\n",
    "#i1 = torch.tensor(self.i1, requires_grad=True)\n",
    "#i2 = torch.tensor(self.i2, requires_grad=True)\n",
    "#c1 = torch.tensor(self.c1, requires_grad=True)\n",
    "#m1 = torch.tensor(self.m1, requires_grad=True)\n",
    "#m2 = torch.tensor(self.m2, requires_grad=True)\n",
    "#m3 = torch.tensor(self.m3, requires_grad=True)\n",
    "#cm1 = torch.tensor(self.cm1, requires_grad=True)\n",
    "#r1 = torch.tensor(self.r1, requires_grad=True)\n",
    "#r2 = torch.tensor(self.r2, requires_grad=True)\n",
    "#max_concurrency = torch.tensor(self.max_concurrency, requires_grad=True)\n",
    "#avg_io_speed = torch.tensor(self.avg_io_speed, requires_grad=True)\n",
    "#memory_size = torch.tensor(self.memory_size, requires_grad=True)\n",
    "#optimizer = optim.Adam([q1, ], lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
